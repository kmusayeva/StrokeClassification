---
title: "Build and deploy a stroke prediction model using R"
author: "Khadija Musayeva"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    self_contained: false
---

<style>
body {
  font-size: 18px;
}
</style>

# About Data Analysis Report

This RMarkdown file contains the report of the data analysis done for the project on building and deploying a stroke prediction model in R. It contains data exploration, data visualization, statistical/epidemiological and predictive modeling analyses of stroke dataset. The final report was completed on `r date()`. 

**Data Description:**

According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.

This data set is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, body mass index, various diseases, and smoking status. Each row in the data provides relevant information about the patient.


## Data preprocessing and analysis

### Install and load packages

```{r message=FALSE, warning=FALSE}
### we use pacman to install and load the required packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load("caret", "data.table", "DescTools", "epitools", "GGally", "ggplot2", "gridExtra", "mlbench", "mltools", "naniar", "parsnip", "pROC", "ranger", "reshape2", "recipes", "rsample", "shiny", "smotefamily", "themis","tidymodels", "tune", "viridis", "workflows", "yardstick", "xgboost")

source("utils.R")

```



### Load the data

Read the data and check its dimensions:

```{r}
dat <- as.data.frame(read.csv("healthcare-dataset-stroke-data.csv"))
cat("There are", nrow(dat), "samples and", ncol(dat), "input variables in the stroke data.")
```

What are the types of these variables?

```{r}
sapply(dat, class)
```
To this end, we can also use str function, it outputs types of column as well as an overview of some values:

```{r}
str(dat)
```
Most of the variables are categorical. We convert them into factor.

```{r warning=FALSE}
dat <- dat[, -1]

dat[, -c(2, 8, 9)] <- lapply(dat[, -c(2, 8, 9)], as.factor)
  
dat$bmi <- as.numeric(dat$bmi)

```


Let us have a glimpse of the data:

```{r}
head(dat)
```

### **Analysis of missing values**

To analyze the missing values we use naniar package. The following plot shows that 4 percent of bmi values, or 201 of observations have bmi values missing. 

```{r miss-plot, fig.width=7, fig.height=5}
vis_miss(dat)
```

Are these missing values distributed randomly? To this end, we look at the distribution of missing values with respect to the variables of interest. For instance, regarding gender, there are more missing values for males than for females, and regarding the output variable of interest, almost ~16 percent of BMI values for stroke individuals are missing which is at most 4 percent for non-stroke individuals. So these values are missing at random.

```{r miss-distribution-plot, fig.width=15, fig.height=4}

p1 <- plot_missingness_distribution(dat, "gender")

p2 <- plot_missingness_distribution(dat, "stroke")

grid.arrange(p1, p2, ncol = 2)

```



### **Visualization**

The following plots show the distribution of categorical variables. 

```{r cat-plot, fig.width=9, fig.height=9}

# Select categorical variables and stroke column
cat_vars <- dat[, -c(2, 8, 9)]
stroke <- dat$stroke

# convert into long format
long_dat <- cat_vars %>% pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

# change the order of categorical variables for plot
long_dat$variable <- factor(long_dat$variable, levels = c("stroke", "gender", "hypertension",  "heart_disease", "smoking_status", "ever_married", "work_type", "Residence_type")) 

# bar plot of percentages
ggplot(long_dat, aes(x = value, y = after_stat(prop), fill=variable)) +
  geom_bar(position = position_dodge(), stat = "prop")+
  geom_text(aes(label = round(100 * after_stat(prop), 2)),
    position = position_dodge(.9), stat = "prop", vjust = -.2
  ) + 
  facet_wrap(~ variable, nrow = 2, ncol = 4, scales = "free_x") +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(size = 12),
    panel.spacing = unit(1.2, "lines"),
    legend.position="none"
  ) +
  labs(
    x = "",
    y = "Proportion of observations",
    title = ""
  )

```

We see that in this study there are more females than males, more individuals with positive ever-married status, more individuals working in the private sector than the remaining sectors. There are almost the same number of observations in rural and urban categories. Below we look at the prevalence of stroke cases across all categorical variables. 

```{r stroke-plot, fig.width=9, fig.height=6}

cat_vars <- dat[, -c(2, 8, 9, 11)]  # remove continuous vars or unwanted cols
cat_vars$stroke <- dat$stroke  # keep stroke indicator

# convert to long format
long_dat <- cat_vars %>%
  pivot_longer(cols = -stroke, names_to = "variable", values_to = "value")

# stroke prevalence per category
stroke_pct <- long_dat %>%
  group_by(variable, value) %>%
  summarise(
    total = n(),
    stroke_cases = sum(stroke == 1),
    pct = round(100 * stroke_cases / total, 2),
    .groups = "drop"
  )

# set the plot order
stroke_pct$variable <- factor(stroke_pct$variable,
                              levels = c("gender", "hypertension", "heart_disease",
                                         "smoking_status", "ever_married",
                                         "work_type", "Residence_type"))


ggplot(stroke_pct, aes(x = value, y = pct, fill = variable)) +
  geom_bar(stat = "identity") +
  facet_wrap(~variable, nrow = 2, ncol = 4, scales = "free_x") +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, size = 16),
        axis.text.x = element_text(angle = 45, hjust = 1),
        panel.spacing = unit(1.2, "lines"),
        legend.position = "none") +
  labs(x = "", y = "Stroke rate (%)", title = "")


```

We see that both females and males are equally affected by this medical condition.  There are more stroke cases among individuals with hypertension, and heart disease. More formerly-smoked individuals have stroke than the rest, however, there are a lot of individuals with the unknown smoking status, which makes this analysis biased. In general these variables are related to the age. For instance, more formerly-smoked individuals, or ever-married individuals are older people.

We use **Cramer's V** to measure associations between multiple categories. It ranges from 0 (no association) to 1 (perfect association). The following heatmap shows that there is some association between smoking status and marriage status, smoking status and work type. This is likely due to the age of individuals as a confounding variable. Most of the associations, however, are weak. 

```{r}
cat_vars <- dat[, -c(2, 8, 9, 11)] %>%
  mutate(across(everything(), ~ droplevels(as.factor(.))))

cramerV_matrix <- compute_cramer_v_matrix(cat_vars)

cramerV_tidy <- melt(cramerV_matrix, na.rm = TRUE)

cramerV_tidy$Var1 <- factor(cramerV_tidy$Var1, levels = rownames(cramerV_matrix))

cramerV_tidy$Var2 <- factor(cramerV_tidy$Var2, levels = colnames(cramerV_matrix))


ggplot(cramerV_tidy, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "white", mid = "lightblue", high = "steelblue",
    midpoint = 0.2, limits = c(0, 1), name = "Cramér’s V"
  ) +
  geom_text(aes(label = round(value, 2)), color = "black", size = 3) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid = element_blank()
  ) +
  labs(title = "", x = "", y = "" )

```

What about continuous variables: age, average glucose level, and body mass index?

The majority of individuals have an average glucose level below 150, but there is a small group of individuals whose average glucose level is concentrated around ~210.

```{r hist-plots, fig.height=3, fig.width=9, message=FALSE, warning=FALSE}

p1 <- ggplot(dat, aes(x=avg_glucose_level)) +  
      geom_histogram(fill="#92C5DE")+
      labs(x = "Average glucose level", y = "") +
      theme_minimal(base_size = 12)+
      theme(legend.position="none")


p2 <- ggplot(dat, aes(x=bmi)) +  
      geom_histogram(fill="#E69F00")+
      labs(x = "Body mass index", y = "") +
      theme_minimal(base_size = 12)+
      theme(legend.position="none")


p3 <- ggplot(dat, aes(x=age)) +  
      geom_histogram(fill="#D6604D")+
      labs(x = "Age", y = "") +
      theme_minimal(base_size = 12)+
      theme(legend.position="none")


grid.arrange(p1, p2, p3, ncol=3)

```

The distribution of average glucose level among individuals who experienced a stroke is visibly multimodal, with a secondary peak above 200. The median average glucose level among stroke cases is around 105. **While individuals with average glucose levels above ~ 200mg/dL constitute a small portion of observations, they account for approximately 25% of all stroke cases,** as the following plot shows.

```{r glucose-stroke-hist, fig.height=4, fig.width=9, warning=FALSE}

p1 <- ggplot(dat, aes(x = stroke, y = avg_glucose_level, color=stroke)) + 
      geom_boxplot(outlier.shape = NA) +
      geom_jitter(width = 0.2, alpha = 0.6, size = 1.5)+
      scale_fill_brewer(palette = "Set3") +
      scale_color_brewer(palette = "Set2") +
      labs(x = "Stroke", y = "Average glucose level", color = "Stroke") +
      theme_minimal(base_size = 14) +
      theme(legend.position = "none")  


agl_means <- dat %>% group_by(stroke) %>% summarise(agl_mean = median(avg_glucose_level)) 

p2 <- ggplot(dat, aes(x=avg_glucose_level, fill=stroke)) +  geom_density(alpha=0.4) +
      geom_vline(data = agl_means, aes(xintercept=agl_mean, color=stroke), linetype="dashed")+
      labs(x = "Average glucose level", y = "Density", color = "stroke") +
      scale_fill_brewer(palette = "Set3") +
      scale_color_brewer(palette = "Set2") +
      theme_minimal(base_size = 14) 
      #+
      #theme(legend.position = "none")


grid.arrange(p1, p2, ncol=2)


```


The distribution of body mass index is right skewed. For BMI variable there is much overlap between the stroke and non-stroke cases, but the one for stroke individuals is slightly shifted to the right.

```{r bmi-stroke-hist, fig.height=4, fig.width=10, warning=FALSE}

p1 <- ggplot(dat, aes(x = stroke, y = bmi, color=stroke)) + 
      geom_boxplot(outlier.shape = NA) +
      geom_jitter(width = 0.2, alpha = 0.6, size = 1.5)+
      scale_fill_brewer(palette = "Set3") +
      scale_color_brewer(palette = "Set2") +
      labs(x = "Stroke", y = "Body mass index", color = "Stroke") +
      theme_minimal(base_size = 14) +
      theme(legend.position = "none")  


bmi_means <- dat %>% filter(!is.na(dat$bmi)) %>%  group_by(stroke) %>% summarise(bmi_mean = mean(bmi)) 

p2 <- ggplot(dat, aes(x=bmi, fill=stroke)) +  geom_density(alpha=0.4)+
      geom_vline(data = bmi_means, aes(xintercept=bmi_mean, color=stroke), linetype="dashed")+
      labs(x = "Body mass index", y = "Density", color="stroke") +
      scale_fill_brewer(palette = "Set3") +
      scale_color_brewer(palette = "Set2") +
      theme_minimal(base_size = 14)

grid.arrange(p1, p2, ncol=2)

```


The following plots show that the age is clearly a risk factor for stroke: **75% of individuals with stroke are aged above ~60**, whereas **75% of individuals with no stroke are aged below ~60. For stroke individuals, the distribution is left skewed with half of strokes happening above the age 70.** There are no stroke cases between the ages 20 and 30 in the data, but there are 2 children with stroke. The cases involving children can be considered special cases.

```{r age-stroke-plot, fig.height=4, fig.width=9, warning=FALSE}

p1 <- ggplot(dat, aes(x = stroke, y = age, color=stroke)) + 
      geom_boxplot(outlier.shape = NA) +
      geom_jitter(width = 0.2, alpha = 0.6, size = 1.5)+
      scale_fill_brewer(palette = "Set3") +
      scale_color_brewer(palette = "Set2") +
      labs(x = "Stroke", y = "Age", color = "Stroke") +
      theme_minimal(base_size = 14) +
      theme(legend.position = "none")  


age_means <- dat %>% group_by(stroke) %>% summarise(age_mean = mean(age)) 

p2 <- ggplot(dat, aes(x=age, fill=stroke)) +  geom_density(alpha=0.4)+
      geom_vline(data = age_means, aes(xintercept=age_mean, color=stroke), linetype="dashed")+
      labs(x = "Age", y = "Density", color = "Stroke") +
      scale_fill_brewer(palette = "Set3") +
      scale_color_brewer(palette = "Set2") +
      theme_minimal(base_size = 14) +
      theme(legend.position = "none")

grid.arrange(p1, p2, ncol=2)

```

Next, we analyze pairwise relationships between these variables. **We see that for stroke individuals, there is a negative correlation between the age and the body mass index. This association is positive for non-stroke cases. This is because the majority of strokes happens above the age of 70, where body mass index starts to decline.** The association between the average glucose level and body mass index is higher than it is in the non-stroke case.

```{r pairwise-plot, fig.height=5, fig.width=10, warning=FALSE}

ggpairs(dat, columns=c(2, 8, 9), aes(color=stroke, alpha=0.3),
        lower=list(continuous="smooth"), diag=list(continuous="densityDiag"))+
  scale_fill_brewer(palette = "Set3") +
  scale_color_brewer(palette = "Set2")

```

**75%** of the hypertension individuals are above the age of **~52** and **half** of the hypertension individuals are above the age of **~62**. **Stroke appears later in life, regardless of hypertension, however the hypertension individuals who experienced stroke are generally older individuals**.

```{r pairwise_age_hypertension, fig.height=5, fig.width=15, warning=FALSE}

p1 <- ggplot(dat, aes(x = hypertension, y = age, fill = hypertension)) + 
      geom_boxplot(outlier.shape = NA, position = position_dodge(0.8)) +
      geom_jitter(aes(color=hypertension), width=0.2, alpha = 0.5, size = 1.2) +
      scale_fill_brewer(palette = "Set2") +
      scale_color_brewer(palette = "Set2") +
      labs(x = "Hypertension", y = "Age", title = "Age distribution by hypertension") +
      theme_minimal(base_size = 14)+
      theme(plot.title = element_text(hjust = 0.5, size = 16))


p2 <- ggplot(dat, aes(x = hypertension, y = age, fill = stroke)) + 
      geom_boxplot(outlier.shape = NA, position = position_dodge(0.8)) +
      geom_jitter(aes(color = stroke), 
                  position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.8),
                  alpha = 0.5, size = 1.2) +
      scale_fill_brewer(palette = "Set3") +
      scale_color_brewer(palette = "Set2") +
      labs(x = "Hypertension", y = "Age", fill = "Stroke", color = "Stroke", 
           title = "Age distribution by hypertension and stroke") +
      theme_minimal(base_size = 14)+
      theme(plot.title = element_text(hjust = 0.5, size = 16))

grid.arrange(p1, p2, ncol=2)

```

The median age for heart disease is higher than that for hypertension: above 70. Similarly, **stroke appears later in life, regardless of heart disease, however there is less variability in the age distribution of individuals with heart disease and stroke: they are predominantly older.**

```{r pairwise_age_heart_disease, fig.height=5, fig.width=15, warning=FALSE}

p1 <- ggplot(dat, aes(x = heart_disease, y = age, fill = heart_disease)) + 
      geom_boxplot(outlier.shape = NA, position = position_dodge(0.8)) +
      geom_jitter(aes(color=heart_disease), width=0.2, alpha = 0.5, size = 1.2) +
      scale_fill_brewer(palette = "Set2") +
      scale_color_brewer(palette = "Set2") +
      labs(x = "Heart disease", y = "Age", title = "Age distribution by heart disease") +
      theme_minimal(base_size = 14)+
      theme(plot.title = element_text(hjust = 0.5, size = 16))


p2 <- ggplot(dat, aes(x = heart_disease, y = age, fill = stroke)) + 
      geom_boxplot(outlier.shape = NA, position = position_dodge(0.8)) +
      geom_jitter(aes(color = stroke), 
                  position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.8),
                  alpha = 0.5, size = 1.2) +
      scale_fill_brewer(palette = "Set3") +
      scale_color_brewer(palette = "Set2") +
      labs(
        x = "Heart disease",
        y = "Age",
        fill = "Stroke",
        color = "Stroke",
        title = "Age distribution by heart disease and stroke"
      ) +
      theme_minimal(base_size = 14)+
      theme(
         plot.title = element_text(hjust = 0.5, size = 16)
      )

grid.arrange(p1, p2, ncol=2)

```

The following plot shows the age distribution across combinations of hypertension and heart diseases. For instance 1_1 indicates the individuals who had both hypertension and heart disease. 

```{r pairwise_age_heartd_hypert, fig.height=5, fig.width=15, warning=FALSE}

stroke_hypert_heartd <- dat %>%mutate(hyp_hd = interaction(hypertension, heart_disease, sep = "_"))

ggplot(stroke_hypert_heartd, aes(x = hyp_hd, y = age, fill = stroke)) +
    geom_boxplot(position = position_dodge(width = 0.75), outlier.shape = NA) +
    geom_jitter(aes(color = stroke),  
                position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.75),
                alpha = 0.4, size = 1.2) +
    scale_fill_brewer(palette = "Set3") +
    scale_color_brewer(palette = "Set2") +
    labs(
      x = "Hypertension_heart disease",
      y = "Age",
      title = "Age distribution by hypertension & heart disease, split by stroke"
    ) +
    theme_minimal(base_size = 14) +
    theme(plot.title = element_text(hjust = 0.5, size = 16))


```



### **Epidemiological analysis**

In this section, we will do some statistical analysis of association between variables. 

#### **Odds ratio**

To compute the odds ratio we use **epitools package**. 
The outcome of interest is stroke, and the exposures are: hypertension, heart disease, average glucose level, age and gender.

The following odds ratio computation shows that the **odds of stroke in hypertensive individuals is ~3.7 times higher than that in non-hypertensive individuals**. 

```{r oods-ratio-hypertension}

tbl_hypertension <- table(dat$stroke, dat$hypertension)

oddsratio(tbl_hypertension)

```


Also the **odds of stroke in individuals with heart disease is ~4.71 times higher than that in individuals with no heart disease.** 
```{r odds-ratio-heart-disease}

tbl_heart_disease <- table(dat$stroke, dat$heart_disease)

oddsratio(tbl_heart_disease)

```


To do odds ratio analysis with average glucose level, we bin it into two groups at the level of 180 mg/dL. **Individuals with average glucose level >180 have ~4 times higher odds of stroke than those with glucose ≤180.**

```{r odds-ratio-glucose}

stroke_agl <- dat %>% mutate(agl_group = cut(avg_glucose_level, breaks = c(0, 180, max(dat$avg_glucose_level)))) %>% select(c(stroke, agl_group))

tbl_agl <- table(stroke_agl$stroke, stroke_agl$agl_group)

oddsratio(tbl_agl)

```


We also consider the odds ratio for body mass index, where we group the individuals into the normal weight, i.e., below 25 kg/m2, and overweight, i.e., above 25 kg/m2, categories. The result shows that **overweight individuals have ~2.24 times higher odds of stroke than those with normal weight.**

```{r body-mass-index}
dat_bmi <- dat[!is.na(dat$bmi),]

dat_bmi_stroke <- dat_bmi %>% mutate(g = cut(bmi, breaks = c(0, 25, max(dat_bmi$bmi)))) %>% select(c(stroke, g))

tbl_bmi <- table(dat_bmi_stroke$stroke, dat_bmi_stroke$g)

oddsratio(tbl_bmi)

```


**Odds of stroke in individuals aged above 60 is ~8.13 times higher than that in indivduals aged less than 60.**
Thus we see that **age is a very important risk factor**. This result is **statistically very significant, the confidence interval does not contain 1**, as the two result reported above.

```{r odds-ratio-age}

stroke_age <- dat %>% mutate(age_group = cut(age, breaks = c(0, 60, max(dat$age)), labels=c("0", "1"))) %>% select(c(stroke, age_group))

tbl_age <- table(stroke_age$stroke, stroke_age$age_group)

oddsratio(tbl_age)

```

Now, how about the gender? **There is no association between gender and the occurrence of stroke.**

```{r odd-ratio-gender, message=FALSE, warning=FALSE}

tbl_gender <- table(dat$stroke, dat$gender)

oddsratio(tbl_gender)

``` 


#### **Attributable risk**

What percentage of the stroke cases in the hypertension individuals can be attributed to hypertension? To answer this question we need to compute attributable risk percent: **70% of strokes in hypertension individuals could be attributed to hypertension.**

```{r attri-risk-hypertension}

total_hypert <- sum(dat$hypertension==1)
total_no_hypert <- sum(dat$hypertension==0)

stroke_no_hypertension <- sum((dat$hypertension==0) & (dat$stroke==1))
stroke_with_hypertension <- sum((dat$hypertension==1) & (dat$stroke==1))

par <- round(((stroke_with_hypertension/total_hypert)-(stroke_no_hypertension/total_no_hypert))*100 / (stroke_with_hypertension/total_hypert))

cat("Attributable risk percent due to hypertension is", par, ".")

```

What percentage of the stroke cases in individuals above the age of 60 can be attributed to their age?  **86% of strokes in individuals above the age of 60 could be attributed to their age.**

```{r attri-risk-age}

total_above_60 <- sum(stroke_age$age_group==1)
total_below_60 <- sum(stroke_age$age_group==0)

stroke_above_60 <- sum((stroke_age$age_group==1) & (dat$stroke==1))
stroke_below_60 <- sum((stroke_age$age_group==0) & (dat$stroke==1))

par <- round(((stroke_above_60/total_above_60)-(stroke_below_60/total_below_60))*100 / (stroke_above_60/total_above_60))

cat(paste0("Attributable risk percent due to being above 60 is ", par, "%."))

```


#### **Population attributable risk**

What percentage of the stroke cases in the data can be attributed to hypertension? To answer this question we need to compute population attributable risk percent. The computation below shows that only **19% of all stroke cases could be attributed to hypertension**. In a similar vein, only **14% of all stroke cases could be attributed to heart disease**.

```{r population-attri-risk-hypertension}

n <- nrow(dat)
total_stroke <- sum(dat$stroke==1)
no_hypert <- sum(dat$hypertension==0)
stroke_no_hypertension <- sum((dat$hypertension==0) & (dat$stroke==1))

no_heart_disease <- sum(dat$heart_disease==0)
stroke_no_heart_disease <- sum((dat$heart_disease==0) & (dat$stroke==1))


par <- round(((total_stroke/n)-(stroke_no_hypertension/no_hypert))*100/(total_stroke/n))
par2 <- round(((total_stroke/n)-(stroke_no_heart_disease/no_heart_disease))*100/(total_stroke/n))

cat("Population attributable risk percent due to hypertension is", par, "and population attributable risk percent due to heart disease is", par2, ".")

```


#### **Odds of stroke after adjusting for all variables**

Logistic regression models the log odds of stroke as a linear combination of variables. It allows us to see the effect of each covariate on the odds of stroke while adjusting for all the other covariates. The result below shows that the age, average glucose level, and hypertension are significant covariates for stroke, but heart disease is not, after adjusting for other variables. In other words, **older individuals, individuals with hypertension, and individuals with higher glucose levels are at increased risk of stroke.**

```{r warning=FALSE}

lg_model <- glm(stroke ~ age + gender + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level  + smoking_status, data = dat, family = "binomial")

summary(lg_model)

```

To interpret the effect of age, hypertension, and average glucose level on stroke, we need to exponentiate the coefficients. We see that **each additional year is associated with the ~7.7% increase in the odds of stroke. Each additional average glucose level is associated with the ~0.4% increase in the odds of stroke. People with hypertension have ~1.5 times the odds of stroke compared to those without hypertension.**

```{r warning=FALSE}

coeffs <- lg_model$coefficients

coeffs_sign <- coeffs[names(coeffs) %in% c("age", "hypertension1", "avg_glucose_level")]

exp(coeffs_sign)

```


# Task Three: Evaluate and select prediction models

### **First thing first: What evaluation measures?**

This is a problem with high class imbalance: only around 5 percent of observations are stroke cases. Thus it is important to choose relevant evaluation measures. Without any learning, without even considering any input variables, predicting no stroke would achieve a very high accuracy result. Thus we concentrate on **sensitivity** and **specificity** first. **Sensitivity measures the proportion of individuals correctly identified as stroke cases among those who actually have a stroke, and specificity measures the proportion of individuals correctly identified as healthy among those who are actually healthy.** These measures take values between O and 1. Now, if all individuals are predicted as non-stroke ones, then the specificity will be 1 since there is no false positives. However, sensitivity in this case will be 0, since 0 stroke case, i.e., 0 true positive is identified.

Sensitivity is also known as recall. Precision computes the fraction of true positives over the sum of true and false positives. The **F1-measure** is the harmonic mean of precision and recall, it balances precision and recall. But notice that if the specificity is 1 and the sensitivity is 0, then the precision is undefined, since there is neither true positives or false positives predicted. Thus, F1 measure is not defined.

Next, we consider the **area under the ROC curve**. This curve draws sensitivity against false positive rate (1-specificity) at every cut-off threshold level for predicted probability values, and the larger the area under this curve the better the predictive performance of algorithm.

Finally, we consider **Matthews correlation coefficient. The value of this metric is computed based on the entire confusion matrix, i.e., it takes into account true positives, false positives, true negatives and false negatives. It can be considered the most suitable measure in the class-imbalance setting.**


Now, since we are dealing with the class-imbalance problem, we can choose cut-off threshold values below the standard 0.5 level. This will output different values for sensitivity, specificity, F1-measure and Matthews coefficient. Our strategy is based on the Youden's index, i.e., the threshold that maximizes sensitivity + specificity - 1.


### **Missing values, data splitting**

We  can impute the missing values using the k-nearest neighbour method. It means that the individuals having similar characteristics will have similar body mass index. But it is questionable from precision medicine point of view. We can as well remove these individuals from the data set on the basis of biological differences between individuals even with very similar characteristics. 

We first split the data into the 75%/25% training and test sets using rsample package of tidymodels framework. **We do stratification based on the stroke variable to ensure that the proportion of stroke cases is similar across these splits.** 
In what follow, we use set.seed to ensure reproducibility.

```{r data-split}

set.seed(123)

dat_split <- initial_split(dat, prop = 3/4, strata = stroke)

dat_train <- training(dat_split)

dat_test <- testing(dat_split)
                      
dat_cv <- vfold_cv(dat_train)

```



### **Predictive modeling**

We use logistic regression, also models of higher capacity, capable of learning non-linear decision boundaries, such as random forest and an improved version of the gradient boosting method, extreme gradient boosting. The latter models are particularly suitable for data with many categorical variables.

We train logistic regression with and without SMOTE, and the remaining models with SMOTE. The latter generates synthetic samples for the rare class based on the k-nearest neighbour method. The proportion of synthetic examples can be controlled thanks to the over_ratio option in the recipe function. The number of synthetic samples generated affect sensitivity and specificity: higher number of synthetic samples will improve sensitivity, and degrade specificity.

We use parsnip package of tidymodels to train logistic regression model and random forest. There is no implementation of extreme gradient boosting, xgb, in tidymodels, so we use xgboost package. We do a grid search for hyperparameter tuning.

#### **Logistic regression**

```{r}

ground_truth <- factor(dat_test$stroke, levels = c(1, 0)) # set the ground truth

dat_recipe <- recipe(stroke ~ ., data = dat_train) %>% step_impute_knn(all_predictors())

lr_model <- logistic_reg() %>%  set_engine("glm") %>% set_mode("classification") 

lr_workflow <- workflow() %>% add_model(lr_model) %>% add_recipe(dat_recipe)

lr_fit <- fit(lr_workflow, data = dat_train)

lr_fit_extracted <- extract_fit_engine(lr_fit)

lr_probs1 <- predict(lr_fit, dat_test, type = "prob")

lr_result1 <- evaluate_model_fit(ground_truth, lr_probs1$.pred_1)

```

and train with SMOTE:

```{r}

dat_smote_recipe <- recipe(stroke ~ ., data = dat_train) %>%
  step_impute_knn(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_smote(stroke, over_ratio = 0.5)

lr_model2 <- logistic_reg() %>%  set_engine("glm") %>% set_mode("classification") 

lr_workflow2 <- workflow() %>% add_model(lr_model2) %>% add_recipe(dat_smote_recipe) ## use smote data recipe

lr_fit2 <- fit(lr_workflow2, data = dat_train)

lr_fit_extracted2 <- extract_fit_engine(lr_fit2)

lr_probs2 <- predict(lr_fit2, dat_test, type = "prob")

lr_result2 <- evaluate_model_fit(ground_truth, lr_probs2$.pred_1)

```


#### **Random forest**

```{r}

rf_model <- rand_forest() %>% set_args(mtry = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification") 


rf_workflow <- workflow() %>%
  add_recipe(dat_smote_recipe) %>%
  add_model(rf_model)

rf_grid <- expand.grid(mtry = c(2, 3, 4, 5))

rf_tune_results <- rf_workflow %>% tune_grid(resamples = dat_cv, grid = rf_grid, metrics = metric_set(roc_auc))

rf_tune_results %>%  collect_metrics()

param_final <- rf_tune_results %>% select_best(metric = "roc_auc")

rf_workflow <- rf_workflow %>% finalize_workflow(param_final)

rf_fit <- rf_workflow %>% last_fit(dat_split)

#test_performance <- rf_fit %>% collect_metrics()

rf_probs <- as.data.frame(rf_fit$.predictions)$.pred_1

rf_result <- evaluate_model_fit(ground_truth, rf_probs)

```


#### **Extreme gradient boosting**

```{r}
dat_smote_recipe <- dat_smote_recipe %>% prep()

X_train <- dat_train %>% select(-stroke)
X_test <- dat_test %>% select(-stroke)

X_train <- bake(dat_smote_recipe, X_train)
X_test <- bake(dat_smote_recipe, X_test)

y_train <- as.numeric(dat_train$stroke)-1
y_test  <- as.numeric(dat_test$stroke)-1


X_train_matrix <- as.matrix(X_train)
X_test_matrix <- as.matrix(X_test)

dtrain <- xgb.DMatrix(data = X_train_matrix, label = y_train)
dtest  <- xgb.DMatrix(data = X_test_matrix, label = y_test)


params_list <- expand.grid(
  eta = c(0.01, 0.1, 0.3),
  max_depth = c(3, 6, 9, 12),
  subsample = c(0.3, 0.5, 0.7, 0.9),
  colsample_bytree = c(0.2, 0.5, 0.8)
)

cv_results <- xgb_tune(params_list, dtrain, 100)

cv_summary <- rbindlist(cv_results)

best_params <- cv_summary[which.max(auc)]

### we train xgb with the best hyperparameter configuration on the entire training set.
xgb_final_model <- xgb.train(
  params = as.list(best_params),
  data = dtrain,
  nrounds = best_params$best_iteration,  
  watchlist = list(train = dtrain),
  verbose = 0
  )

xgb_probs <- predict(xgb_final_model, dtest)

xgb_result <- evaluate_model_fit(ground_truth, xgb_probs)


```



### **Performance comparison of models**

**According to Matthiews correlation coefficient, all methods perform weakly (better than random)**. Their performances are comparable, but logistic regression with smote stands out in terms of AUC, F1-measure, Matthews coefficient and specificity. We see that generating synthetic examples overall improved the predictive performance of logistic regression model at the cost of sensitivity. 

```{r fig.height=4, fig.width=10}

results_df <- tribble(
  ~Model,         ~Sensitivity, ~Specificity, ~F1,       ~AUC,      ~Matthews,
  "Logistic Reg", lr_result1$sensi, lr_result1$speci, lr_result1$f1, lr_result1$aucroc, lr_result1$matcc,
  "Logistic Reg Smote", lr_result2$sensi, lr_result2$speci, lr_result2$f1, lr_result2$aucroc, lr_result2$matcc,
  "Random Forest Smote", rf_result$sensi, rf_result$speci, rf_result$f1, rf_result$aucroc, rf_result$matcc,
  "XGBoost Smote",       xgb_result$sensi, xgb_result$speci, xgb_result$f1, xgb_result$aucroc, xgb_result$matcc
)


results_long <- results_df %>% pivot_longer(cols = -Model, names_to = "Metric", values_to = "Value")


ggplot(results_long, aes(x = Metric, y = Value, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge(), width = 0.7) +
  scale_fill_brewer(palette = "Set2") +
  scale_color_brewer(palette = "Set2") +
  labs(
    title = "Performance comparison",
    y = "Value",
    x = ""
  ) +
  theme_minimal()+     
  theme(plot.title = element_text(hjust = 0.5, size = 16))

```


```{r fig.height=7, fig.width=25, warning=FALSE}

p1 <- plot_roc(data.frame(ground_truth, probs=lr_probs2$.pred_1), "Logistic Regression")

p2 <-  plot_roc(data.frame(ground_truth, probs=rf_probs), "Random Forest")

p3 <-  plot_roc(data.frame(ground_truth, probs=xgb_probs), "XGB")

grid.arrange(p1, p2, p3, ncol = 4)

```

From the confusion matrix, we see that a lot of individuals are predicted as the stroke case, since we chose lower cut-off threshold from the ROC curve. Logistic regression has smaller number of false positives.

```{r fig.height=5, fig.width=15, warning=FALSE}

p1 <- plot_confusion_matrix(ground_truth, lr_result2$pred_class, round(lr_result2$matcc, 2), "Logistic regression")

p2 <- plot_confusion_matrix(ground_truth, rf_result$pred_class, round(rf_result$matcc, 2), "Random forest")

p3 <- plot_confusion_matrix(ground_truth, xgb_result$pred_class, round(xgb_result$matcc, 2), "XGB")


grid.arrange(p1, p2, p3, ncol = 3)

```


### **Incorrect predictions of logistic regression**

Now let us look at the distribution of incorrect predictions for logistic regression (trained with smote). We consider important variables such as age and average glucose level. Since the age is a very important risk factor for stroke, as expected, there are many false positives for older age. There are many false positivies across all values of average glucose but they are mostly concetrated around its two modes.

```{r incorrect-preds, fig.height=5, fig.width=15, warning=FALSE}

preds <- lr_result2$pred_class

dat_test_preds <- dat_test

dat_test_preds$preds <- factor(preds, levels=c(1,0))

dat_test_preds$outcome <- with(dat_test_preds, ifelse(
  stroke == 1 & preds == 1, "True Positive",
  ifelse(stroke == 0 & preds == 0, "True Negative",
  ifelse(stroke == 0 & preds == 1, "False Positive", "False Negative"))))


p1 <- plot_wrong_predictions(dat_test_preds, "age", "Age")

p2 <- plot_wrong_predictions(dat_test_preds, "avg_glucose_level", "Average glucose level")


grid.arrange(p1, p2, ncol=2)

```

**Positive predictive value and negative predictive value of logistic regression**

The positive predictive value (PPV) is the probability that an individual predicted to have a stroke truly has a stroke, while the negative predictive value (NPV) is the probability that an individual predicted not to have a stroke truly does not have a stroke. Considering our logistic regression model as a diagnostic tool, the probability that an individual predicted to have a stroke has a stroke is 16% and the probability that an individual predicted not to have a stroke does not have a stroke is 98%.

```{r}

cm <- confusionMatrix(lr_result2$pred_class, ground_truth, positive = "1")
metrics <- cm$byClass
ppv <- metrics[3]
npv <- metrics[4]

cat("Positive predicted value is", round(ppv,2), "and negative predicted value is", round(npv, 2))

```



# Task Four: Deploy the prediction model

For deployment, we train logistic regression on the entire dataset, where we impute the missing values of BMI and generate synthetic examples of the rare class. We save the trained model, and use it in shiny interface for which we have created in a separate file.

```{r}

final_data <- dat

final_recipe <- recipe(stroke ~ ., data = final_data) %>%
  step_impute_knn(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_smote(stroke, over_ratio = 0.5)


final_workflow <- workflow() %>% 
  add_model(lr_model2) %>% 
  add_recipe(final_recipe)


final_fit <- fit(final_workflow, data = final_data)

saveRDS(final_fit, file = "stroke_model.rds")

```


# Task Five: Findings and Conclusions

The prevalence of stroke cases is ~5%, making this a highly imbalanced dataset from a classification perspective. Data visualization revealed considerable overlap between the class-conditional distributions of important variables such as body mass index, average glucose level, heart disease, and age. This indicates that no single covariate is clearly discriminative for stroke classification. From precision medicine point of view, incorporating family history, ethnic origin, lifestyle, other environmental factors such as geographic location, and genomic information might improve the classification of stroke cases.

Statistical analysis showed that age is a particularly important risk factor: the majority of stroke cases occur above the age of ~60. Other important predictors are hypertension and average glucose level. In contrast, stroke occurrs at similar rates across male and female individuals.

To address the class imbalance problem, we applied SMOTE to synthetically generate examples of the stroke class and we used a lower-than-standard probability threshold (less than 0.5) to classify strokes. These manipulations increase model sensitivity at the cost of more false positives.  We used a parametric method such as logistic regression, and tree-based models capable of learning highly non-linear decision boundaries and well adapted to data with many categorical variable: random forest and gradient boosting. All models are comparable regarding their predictive performance: they are better than a random guess according to Matthews correlation coefficient which is computed based on the whole confusion matrix. However logistic regression stood out with respect to AUC, F1 measures, Matthews correlation coefficient and specificity.

There is a clear trade-off between sensitivity and specificity: improving sensitivity results in more false positives and some false negatives. For example, adjusting the classification threshold alters the balance between these metrics. This is an important  epidemiological question: what are the consequences of generating many false positives in order to detect a rare but serious condition like stroke?





























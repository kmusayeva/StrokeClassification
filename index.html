<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Khadija Musayeva" />

<meta name="date" content="2025-04-15" />

<title>Build and deploy stroke prediction model using R</title>

<script src="Build-deploy-stroke-prediction-model-R_files/header-attrs-2.29/header-attrs.js"></script>
<script src="Build-deploy-stroke-prediction-model-R_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="Build-deploy-stroke-prediction-model-R_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="Build-deploy-stroke-prediction-model-R_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="Build-deploy-stroke-prediction-model-R_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="Build-deploy-stroke-prediction-model-R_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="Build-deploy-stroke-prediction-model-R_files/navigation-1.1/tabsets.js"></script>
<link href="Build-deploy-stroke-prediction-model-R_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="Build-deploy-stroke-prediction-model-R_files/highlightjs-9.12.0/highlight.js"></script>
<script src="Build-deploy-stroke-prediction-model-R_files/kePrint-0.0.1/kePrint.js"></script>
<link href="Build-deploy-stroke-prediction-model-R_files/lightable-0.0.1/lightable.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div id="header">



<h1 class="title toc-ignore">Build and deploy stroke prediction model
using R</h1>
<h4 class="author">Khadija Musayeva</h4>
<h4 class="date">2025-04-15</h4>

</div>


<style>
body {
  font-size: 18px;
}
</style>
<div id="about-data-analysis-report" class="section level1">
<h1>About Data Analysis Report</h1>
<p>This RMarkdown file contains the report of the data analysis done for
the project on building and deploying stroke prediction model in R. It
contains data exploration, data visualization,
statistical/epidemiological and predictive modeling analyses of stroke
dataset. The final report was completed on Tue Apr 15 15:48:11 2025.</p>
<p><strong>Data Description:</strong></p>
<p>According to the World Health Organization (WHO) stroke is the 2nd
leading cause of death globally, responsible for approximately 11% of
total deaths.</p>
<p>This data set is used to predict whether a patient is likely to get
stroke based on the input parameters like gender, age, body mass index,
various diseases, and smoking status. Each row in the data provides
relevant information about the patient.</p>
<div id="data-preprocessing-and-analysis" class="section level2">
<h2>Data preprocessing and analysis</h2>
<div id="install-and-load-packages" class="section level3">
<h3>Install and load packages</h3>
<pre class="r"><code>### we use pacman to install and load the required packages
if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;)
pacman::p_load(&quot;caret&quot;, &quot;data.table&quot;, &quot;DescTools&quot;, &quot;dials&quot;, &quot;egg&quot;, &quot;epitools&quot;, &quot;GGally&quot;, &quot;ggplot2&quot;, &quot;gridExtra&quot;, &quot;kableExtra&quot;, &quot;mlbench&quot;, &quot;mltools&quot;, &quot;naniar&quot;, &quot;parsnip&quot;, &quot;pROC&quot;, &quot;ranger&quot;, &quot;reshape2&quot;, &quot;recipes&quot;, &quot;rsample&quot;, &quot;shiny&quot;, &quot;smotefamily&quot;, &quot;themis&quot;,&quot;tidymodels&quot;, &quot;tune&quot;, &quot;viridis&quot;, &quot;workflows&quot;, &quot;yardstick&quot;, &quot;xgboost&quot;)

source(&quot;utils.R&quot;)</code></pre>
</div>
<div id="load-the-data" class="section level3">
<h3>Load the data</h3>
<p>Read the data and check its dimensions:</p>
<pre class="r"><code>dat &lt;- as.data.frame(read.csv(&quot;healthcare-dataset-stroke-data.csv&quot;))
cat(&quot;There are&quot;, nrow(dat), &quot;samples and&quot;, ncol(dat), &quot;input variables in the stroke data.&quot;)</code></pre>
<pre><code>## There are 5110 samples and 12 input variables in the stroke data.</code></pre>
<p>What are the types of these variables?</p>
<pre class="r"><code>sapply(dat, class)</code></pre>
<pre><code>##                id            gender               age      hypertension 
##         &quot;integer&quot;       &quot;character&quot;         &quot;numeric&quot;         &quot;integer&quot; 
##     heart_disease      ever_married         work_type    Residence_type 
##         &quot;integer&quot;       &quot;character&quot;       &quot;character&quot;       &quot;character&quot; 
## avg_glucose_level               bmi    smoking_status            stroke 
##         &quot;numeric&quot;       &quot;character&quot;       &quot;character&quot;         &quot;integer&quot;</code></pre>
<p>To this end, we can also use str function, it outputs types of column
as well as an overview of some values:</p>
<pre class="r"><code>str(dat)</code></pre>
<pre><code>## &#39;data.frame&#39;:    5110 obs. of  12 variables:
##  $ id               : int  9046 51676 31112 60182 1665 56669 53882 10434 27419 60491 ...
##  $ gender           : chr  &quot;Male&quot; &quot;Female&quot; &quot;Male&quot; &quot;Female&quot; ...
##  $ age              : num  67 61 80 49 79 81 74 69 59 78 ...
##  $ hypertension     : int  0 0 0 0 1 0 1 0 0 0 ...
##  $ heart_disease    : int  1 0 1 0 0 0 1 0 0 0 ...
##  $ ever_married     : chr  &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; ...
##  $ work_type        : chr  &quot;Private&quot; &quot;Self-employed&quot; &quot;Private&quot; &quot;Private&quot; ...
##  $ Residence_type   : chr  &quot;Urban&quot; &quot;Rural&quot; &quot;Rural&quot; &quot;Urban&quot; ...
##  $ avg_glucose_level: num  229 202 106 171 174 ...
##  $ bmi              : chr  &quot;36.6&quot; &quot;N/A&quot; &quot;32.5&quot; &quot;34.4&quot; ...
##  $ smoking_status   : chr  &quot;formerly smoked&quot; &quot;never smoked&quot; &quot;never smoked&quot; &quot;smokes&quot; ...
##  $ stroke           : int  1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<p>Most of the variables are categorical. We convert them into
factor.</p>
<pre class="r"><code>dat &lt;- dat[, -1] ### remove the first id column

dat &lt;- dat[dat$gender!=&quot;Other&quot;,]

dat[, -c(2, 8, 9)] &lt;- lapply(dat[, -c(2, 8, 9)], as.factor)
  
dat$bmi &lt;- as.numeric(dat$bmi)</code></pre>
<p>Let us have a glimpse of the data:</p>
<pre class="r"><code>kable(head(dat), &quot;html&quot;) %&gt;%
  kable_styling(font_size = 12) %&gt;%
  scroll_box(width = &quot;100%&quot;, height = &quot;300px&quot;)</code></pre>
<div
style="border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:300px; overflow-x: scroll; width:100%; ">
<table class="table" style="font-size: 12px; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
gender
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
age
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
hypertension
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
heart_disease
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
ever_married
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
work_type
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
Residence_type
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
avg_glucose_level
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
bmi
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
smoking_status
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
stroke
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Male
</td>
<td style="text-align:right;">
67
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
Yes
</td>
<td style="text-align:left;">
Private
</td>
<td style="text-align:left;">
Urban
</td>
<td style="text-align:right;">
228.69
</td>
<td style="text-align:right;">
36.6
</td>
<td style="text-align:left;">
formerly smoked
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
Female
</td>
<td style="text-align:right;">
61
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
Yes
</td>
<td style="text-align:left;">
Self-employed
</td>
<td style="text-align:left;">
Rural
</td>
<td style="text-align:right;">
202.21
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:left;">
never smoked
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
Male
</td>
<td style="text-align:right;">
80
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
Yes
</td>
<td style="text-align:left;">
Private
</td>
<td style="text-align:left;">
Rural
</td>
<td style="text-align:right;">
105.92
</td>
<td style="text-align:right;">
32.5
</td>
<td style="text-align:left;">
never smoked
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
Female
</td>
<td style="text-align:right;">
49
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
Yes
</td>
<td style="text-align:left;">
Private
</td>
<td style="text-align:left;">
Urban
</td>
<td style="text-align:right;">
171.23
</td>
<td style="text-align:right;">
34.4
</td>
<td style="text-align:left;">
smokes
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
Female
</td>
<td style="text-align:right;">
79
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
Yes
</td>
<td style="text-align:left;">
Self-employed
</td>
<td style="text-align:left;">
Rural
</td>
<td style="text-align:right;">
174.12
</td>
<td style="text-align:right;">
24.0
</td>
<td style="text-align:left;">
never smoked
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
Male
</td>
<td style="text-align:right;">
81
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
Yes
</td>
<td style="text-align:left;">
Private
</td>
<td style="text-align:left;">
Urban
</td>
<td style="text-align:right;">
186.21
</td>
<td style="text-align:right;">
29.0
</td>
<td style="text-align:left;">
formerly smoked
</td>
<td style="text-align:left;">
1
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="analysis-of-missing-values" class="section level3">
<h3><strong>Analysis of Missing Values</strong></h3>
<p>To visualize the distribution of missing values we use the naniar
package.</p>
<p>The following plot shows that 4 percent of bmi values, or 201 of
observations have bmi values missing.</p>
<pre class="r"><code>vis_miss(dat)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/miss-plot-1.png" width="672" /></p>
<p>Are these missing values distributed randomly? To this end, we look
at the distribution of missing values with respect to the variables of
interest. For instance, regarding gender, there are more missing values
for males than for females, and regarding the output variable of
interest, almost ~16 percent of BMI values for stroke individuals are
missing which is at most 4 percent for non-stroke individuals. So these
values are missing at random.</p>
<pre class="r"><code>p1 &lt;- plot_missingness_distribution(dat, &quot;gender&quot;)

p2 &lt;- plot_missingness_distribution(dat, &quot;stroke&quot;)

grid.arrange(p1, p2, ncol = 2)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/miss-distribution-plot-1.png" width="1440" /></p>
</div>
<div id="visualization" class="section level3">
<h3><strong>Visualization</strong></h3>
<p>The following plot illustrates the distribution of categorical
variables.</p>
<p>We see that in this study there are more females than males, more
ever-married individuals, more individuals working in the private sector
than the remaining ones. There are almost the same number of
observations in rural and urban categories.</p>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/cat-plot-1.png" width="864" /></p>
<p><strong>The prevalence of stroke cases across all categorical
variables.</strong></p>
<p>We see that both females and males are equally affected by this
medical condition. There are more stroke cases among individuals with
hypertension, and heart disease. More formerly-smoked individuals have
stroke than the rest, however, there are a lot of individuals with the
unknown smoking status, which makes this analysis biased. Age may be a
confounding variable here. For instance, the majority of formerly-smoked
individuals, or ever-married individuals are relatively older
individuals (above the age of 40).</p>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/stroke-plot-1.png" width="864" /></p>
<p>We use <strong>Cramer’s V</strong> to measure associations between
categories in a single table. It ranges from 0 (no association) to 1
(perfect association). The following heatmap shows that there is some
association between smoking status and marriage status, smoking status
and work type. This is likely due to the age of individuals as a
confounding variable. Most of the associations, however, are weak.</p>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<div id="distribution-of-continuous-variables" class="section level4">
<h4><strong>Distribution of Continuous Variables</strong></h4>
<p>What about the continuous variables: age, average glucose level, and
body mass index?</p>
<p>Half of the individuals have an average glucose level above the
normal 100 mg/dL threshold, indicating a substantial proportion may be
experiencing hyperglycemia. In particular, there is a cluster of
individuals whose glucose levels are concentrated around ~210 mg/dL.
Body mass index distribution is right skewed with more than half of the
individuals having a body mass index (BMI) above the normal upper limit
of 24.9 kg/m^2, i.e., half of the individuals are overweight or obese.
The distribution of age is almost uniform, it includes all ages from
infants to elderly.</p>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/hist-plots-1.png" width="960" /></p>
<p>The distribution of average glucose level among individuals who
experienced stroke is visibly multimodal, with a secondary peak above
200. The median average glucose level among stroke cases is around 105.
<strong>While individuals with average glucose levels above ~ 200mg/dL
constitute a small portion of observations, they account for
approximately 25% of all stroke cases,</strong> as the following plot
shows.</p>
<pre class="r"><code>p1 &lt;- ggplot(dat, aes(x = stroke, y = avg_glucose_level, color=stroke)) + 
      geom_boxplot(outlier.shape = NA) +
      geom_jitter(width = 0.2, alpha = 0.6, size = 1.5)+
      scale_fill_brewer(palette = &quot;Set3&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      labs(x = &quot;Stroke&quot;, y = &quot;Average glucose level&quot;, color = &quot;Stroke&quot;) +
      theme_minimal(base_size = 14) +
      theme(legend.position = &quot;none&quot;)  


agl_means &lt;- dat %&gt;% group_by(stroke) %&gt;% summarise(agl_mean = median(avg_glucose_level)) 

p2 &lt;- ggplot(dat, aes(x=avg_glucose_level, fill=stroke)) +  geom_density(alpha=0.4) +
      geom_vline(data = agl_means, aes(xintercept=agl_mean, color=stroke), linetype=&quot;dashed&quot;)+
      labs(x = &quot;Average glucose level&quot;, y = &quot;Density&quot;, color = &quot;stroke&quot;) +
      scale_fill_brewer(palette = &quot;Set3&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      theme_minimal(base_size = 14) 
      #+
      #theme(legend.position = &quot;none&quot;)


grid.arrange(p1, p2, ncol=2)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/glucose-stroke-hist-1.png" width="864" /></p>
<p>For BMI variable there is much overlap between the stroke and
non-stroke cases, but the one for stroke individuals is slightly shifted
to the right.</p>
<pre class="r"><code>p1 &lt;- ggplot(dat, aes(x = stroke, y = bmi, color=stroke)) + 
      geom_boxplot(outlier.shape = NA) +
      geom_jitter(width = 0.2, alpha = 0.6, size = 1.5)+
      scale_fill_brewer(palette = &quot;Set3&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      labs(x = &quot;Stroke&quot;, y = &quot;Body mass index&quot;, color = &quot;Stroke&quot;) +
      theme_minimal(base_size = 14) +
      theme(legend.position = &quot;none&quot;)  


bmi_means &lt;- dat %&gt;% filter(!is.na(dat$bmi)) %&gt;%  group_by(stroke) %&gt;% summarise(bmi_mean = mean(bmi)) 

p2 &lt;- ggplot(dat, aes(x=bmi, fill=stroke)) +  geom_density(alpha=0.4)+
      geom_vline(data = bmi_means, aes(xintercept=bmi_mean, color=stroke), linetype=&quot;dashed&quot;)+
      labs(x = &quot;Body mass index&quot;, y = &quot;Density&quot;, color=&quot;stroke&quot;) +
      scale_fill_brewer(palette = &quot;Set3&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      theme_minimal(base_size = 14)

grid.arrange(p1, p2, ncol=2)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/bmi-stroke-hist-1.png" width="960" /></p>
<p>The following plots show that the age is clearly a risk factor for
stroke: <strong>75% of individuals with stroke are aged above
~60</strong>, whereas <strong>75% of individuals with no stroke are aged
below ~60. For stroke individuals, the distribution is left skewed with
half of strokes happening above the age 70.</strong> There are no stroke
cases between the ages 20 and 30 in the data, but there are 2 children
with stroke. The cases involving children can be considered special
cases.</p>
<pre class="r"><code>p1 &lt;- ggplot(dat, aes(x = stroke, y = age, color=stroke)) + 
      geom_boxplot(outlier.shape = NA) +
      geom_jitter(width = 0.2, alpha = 0.6, size = 1.5)+
      scale_fill_brewer(palette = &quot;Set3&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      labs(x = &quot;Stroke&quot;, y = &quot;Age&quot;, color = &quot;Stroke&quot;) +
      theme_minimal(base_size = 14) +
      theme(legend.position = &quot;none&quot;)  


age_means &lt;- dat %&gt;% group_by(stroke) %&gt;% summarise(age_mean = mean(age)) 

p2 &lt;- ggplot(dat, aes(x=age, fill=stroke)) +  geom_density(alpha=0.4)+
      geom_vline(data = age_means, aes(xintercept=age_mean, color=stroke), linetype=&quot;dashed&quot;)+
      labs(x = &quot;Age&quot;, y = &quot;Density&quot;, color = &quot;Stroke&quot;) +
      scale_fill_brewer(palette = &quot;Set3&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      theme_minimal(base_size = 14) +
      theme(legend.position = &quot;none&quot;)

grid.arrange(p1, p2, ncol=2)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/age-stroke-plot-1.png" width="864" /></p>
<p>Next, we analyze pairwise relationships between these variables split
based on the stroke variable (highlighted as yellow or orange). We see
that individual with highly elevated glucose level are older individuals
and for stroke individuals, there is a negative correlation between the
age and the body mass index. This association is positive for non-stroke
cases. This is because the majority of strokes happens above the age of
70, after which the body mass index starts to decline. The association
between the average glucose level and body mass index is higher in
stroke individuals, than it is in non-stroke individuals, this is due to
the positive relationship of obesity, high glucose level and stroke.</p>
<pre class="r"><code>ggpairs(dat, columns=c(2, 8, 9), aes(color=stroke, alpha=0.3),
        lower=list(continuous=&quot;smooth&quot;), diag=list(continuous=&quot;densityDiag&quot;))+
  scale_fill_brewer(palette = &quot;Set3&quot;) +
  scale_color_brewer(palette = &quot;Set2&quot;)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/pairwise-plot-1.png" width="960" /></p>
<p><strong>75%</strong> of the hypertension individuals are above the
age of <strong>~52</strong> and <strong>half</strong> of the
hypertension individuals are above the age of <strong>~62</strong>.
<strong>Stroke appears later in life, regardless of hypertension,
however the hypertension individuals who experienced stroke are
generally older individuals.</strong></p>
<pre class="r"><code>p1 &lt;- ggplot(dat, aes(x = hypertension, y = age, fill = hypertension)) + 
      geom_boxplot(outlier.shape = NA, position = position_dodge(0.8)) +
      geom_jitter(aes(color=hypertension), width=0.2, alpha = 0.5, size = 1.2) +
      scale_fill_brewer(palette = &quot;Set2&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      labs(x = &quot;Hypertension&quot;, y = &quot;Age&quot;, title = &quot;Age distribution by hypertension&quot;) +
      theme_minimal(base_size = 14)+
      theme(plot.title = element_text(hjust = 0.5, size = 16))


p2 &lt;- ggplot(dat, aes(x = hypertension, y = age, fill = stroke)) + 
      geom_boxplot(outlier.shape = NA, position = position_dodge(0.8)) +
      geom_jitter(aes(color = stroke), 
                  position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.8),
                  alpha = 0.5, size = 1.2) +
      scale_fill_brewer(palette = &quot;Set3&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      labs(x = &quot;Hypertension&quot;, y = &quot;Age&quot;, fill = &quot;Stroke&quot;, color = &quot;Stroke&quot;, 
           title = &quot;Age distribution by hypertension and stroke&quot;) +
      theme_minimal(base_size = 14)+
      theme(plot.title = element_text(hjust = 0.5, size = 16))

grid.arrange(p1, p2, ncol=2)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/pairwise-age-hypertension-1.png" width="1440" /></p>
<p>The median age for heart disease is higher than that for
hypertension: above 70. Similarly, <strong>stroke appears later in life,
regardless of heart disease, however there is less variability in the
age distribution of individuals with heart disease and stroke: they are
predominantly older.</strong></p>
<pre class="r"><code>p1 &lt;- ggplot(dat, aes(x = heart_disease, y = age, fill = heart_disease)) + 
      geom_boxplot(outlier.shape = NA, position = position_dodge(0.8)) +
      geom_jitter(aes(color=heart_disease), width=0.2, alpha = 0.5, size = 1.2) +
      scale_fill_brewer(palette = &quot;Set2&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      labs(x = &quot;Heart disease&quot;, y = &quot;Age&quot;, title = &quot;Age distribution by heart disease&quot;) +
      theme_minimal(base_size = 14)+
      theme(plot.title = element_text(hjust = 0.5, size = 16))


p2 &lt;- ggplot(dat, aes(x = heart_disease, y = age, fill = stroke)) + 
      geom_boxplot(outlier.shape = NA, position = position_dodge(0.8)) +
      geom_jitter(aes(color = stroke), 
                  position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.8),
                  alpha = 0.5, size = 1.2) +
      scale_fill_brewer(palette = &quot;Set3&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      labs(
        x = &quot;Heart disease&quot;,
        y = &quot;Age&quot;,
        fill = &quot;Stroke&quot;,
        color = &quot;Stroke&quot;,
        title = &quot;Age distribution by heart disease and stroke&quot;
      ) +
      theme_minimal(base_size = 14)+
      theme(
         plot.title = element_text(hjust = 0.5, size = 18)
      )

grid.arrange(p1, p2, ncol=2)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/pairwise_age_heart_disease-1.png" width="1440" /></p>
<p>The following plot shows the age distribution across combinations of
hypertension and heart diseases. For instance 1_1 indicates the
individuals who had both hypertension and heart disease.</p>
<pre class="r"><code>stroke_hypert_heartd &lt;- dat %&gt;% mutate(hyp_hd = interaction(hypertension, heart_disease, sep = &quot;_&quot;))

ggplot(stroke_hypert_heartd, aes(x = hyp_hd, y = age, fill = stroke)) +
    geom_boxplot(position = position_dodge(width = 0.75), outlier.shape = NA) +
    geom_jitter(aes(color = stroke),  
                position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.75),
                alpha = 0.4, size = 1.2) +
    scale_fill_brewer(palette = &quot;Set3&quot;) +
    scale_color_brewer(palette = &quot;Set2&quot;) +
    labs(
      x = &quot;Hypertension_heart disease&quot;,
      y = &quot;Age&quot;,
      title = &quot;Age distribution by hypertension &amp; heart disease, split by stroke&quot;
    ) +
    theme_minimal(base_size = 14) +
    theme(plot.title = element_text(hjust = 0.5, size = 18))</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/pairwise_age_heartd_hypert-1.png" width="1440" /></p>
</div>
</div>
<div id="epidemiological-analysis" class="section level3">
<h3><strong>Epidemiological Analysis</strong></h3>
<p>In this section, we do some statistical analysis of association
between variables.</p>
<div id="odds-ratio" class="section level4">
<h4><strong>Odds Ratio</strong></h4>
<p>To compute the odds ratio we use <strong>epitools package</strong>.
The outcome of interest is stroke, and the exposures are: hypertension,
heart disease, average glucose level, age and gender.</p>
<p>The following odds ratio computation shows that the <strong>odds of
stroke in hypertensive individuals is ~3.7 times higher than that in
non-hypertensive individuals</strong>.</p>
<pre class="r"><code>tbl_hypertension &lt;- table(dat$stroke, dat$hypertension)

oddsratio(tbl_hypertension)</code></pre>
<pre><code>## $data
##        
##            0   1 Total
##   0     4428 432  4860
##   1      183  66   249
##   Total 4611 498  5109
## 
## $measure
##    odds ratio with 95% C.I.
##     estimate    lower    upper
##   0 1.000000       NA       NA
##   1 3.700411 2.729038 4.963795
## 
## $p.value
##    two-sided
##      midp.exact fisher.exact   chi.square
##   0          NA           NA           NA
##   1 5.77316e-15 4.592824e-15 6.169579e-20
## 
## $correction
## [1] FALSE
## 
## attr(,&quot;method&quot;)
## [1] &quot;median-unbiased estimate &amp; mid-p exact CI&quot;</code></pre>
<p>Also the <strong>odds of stroke in individuals with heart disease is
~4.71 times higher than that in individuals with no heart
disease.</strong></p>
<pre class="r"><code>tbl_heart_disease &lt;- table(dat$stroke, dat$heart_disease)

oddsratio(tbl_heart_disease)</code></pre>
<pre><code>## $data
##        
##            0   1 Total
##   0     4631 229  4860
##   1      202  47   249
##   Total 4833 276  5109
## 
## $measure
##    odds ratio with 95% C.I.
##     estimate    lower    upper
##   0 1.000000       NA       NA
##   1 4.713009 3.308383 6.600126
## 
## $p.value
##    two-sided
##       midp.exact fisher.exact   chi.square
##   0           NA           NA           NA
##   1 8.881784e-15  7.33664e-15 5.281727e-22
## 
## $correction
## [1] FALSE
## 
## attr(,&quot;method&quot;)
## [1] &quot;median-unbiased estimate &amp; mid-p exact CI&quot;</code></pre>
<p>To do odds ratio analysis with average glucose level, we bin it into
two groups at the level of 180 mg/dL. <strong>Individuals with the
average glucose level &gt;180 have ~4 times higher odds of stroke than
those with the average glucose level ≤180.</strong></p>
<pre class="r"><code>stroke_agl &lt;- dat %&gt;% mutate(agl_group = cut(avg_glucose_level, breaks = c(0, 180, max(dat$avg_glucose_level)))) %&gt;% select(c(stroke, agl_group))

tbl_agl &lt;- table(stroke_agl$stroke, stroke_agl$agl_group)

oddsratio(tbl_agl)</code></pre>
<pre><code>## $data
##        
##         (0,180] (180,272] Total
##   0        4356       504  4860
##   1         170        79   249
##   Total    4526       583  5109
## 
## $measure
##    odds ratio with 95% C.I.
##     estimate    lower    upper
##   0 1.000000       NA       NA
##   1 4.018601 3.016521 5.313551
## 
## $p.value
##    two-sided
##     midp.exact fisher.exact   chi.square
##   0         NA           NA           NA
##   1          0 7.432068e-19 4.756879e-25
## 
## $correction
## [1] FALSE
## 
## attr(,&quot;method&quot;)
## [1] &quot;median-unbiased estimate &amp; mid-p exact CI&quot;</code></pre>
<p>We also consider the odds ratio for body mass index, where we group
the individuals into the normal weight, i.e., below 25 kg/m2, and
overweight, i.e., above 25 kg/m2, categories. The result shows that
<strong>overweight individuals have ~2.24 times higher odds of stroke
than those with normal weight.</strong></p>
<pre class="r"><code>dat_bmi &lt;- dat[!is.na(dat$bmi),]

dat_bmi_stroke &lt;- dat_bmi %&gt;% mutate(g = cut(bmi, breaks = c(0, 25, max(dat_bmi$bmi)))) %&gt;% select(c(stroke, g))

tbl_bmi &lt;- table(dat_bmi_stroke$stroke, dat_bmi_stroke$g)

oddsratio(tbl_bmi)</code></pre>
<pre><code>## $data
##        
##         (0,25] (25,97.6] Total
##   0       1568      3131  4699
##   1         38       171   209
##   Total   1606      3302  4908
## 
## $measure
##    odds ratio with 95% C.I.
##     estimate    lower    upper
##   0 1.000000       NA       NA
##   1 2.245878 1.589959 3.255214
## 
## $p.value
##    two-sided
##       midp.exact fisher.exact   chi.square
##   0           NA           NA           NA
##   1 1.714896e-06 2.443748e-06 4.679649e-06
## 
## $correction
## [1] FALSE
## 
## attr(,&quot;method&quot;)
## [1] &quot;median-unbiased estimate &amp; mid-p exact CI&quot;</code></pre>
<p><strong>Odds of stroke in individuals aged above 60 is ~8.13 times
higher than that in indivduals aged less than 60.</strong> Thus we see
that <strong>age is a very important risk factor</strong>. This result
is <strong>statistically very significant, the confidence interval does
not contain 1</strong>, as the two result reported above.</p>
<pre class="r"><code>stroke_age &lt;- dat %&gt;% mutate(age_group = cut(age, breaks = c(0, 60, max(dat$age)), labels=c(&quot;0&quot;, &quot;1&quot;))) 
tbl_age &lt;- table(stroke_age$stroke, stroke_age$age_group)

oddsratio(tbl_age)</code></pre>
<pre><code>## $data
##        
##            0    1 Total
##   0     3733 1127  4860
##   1       72  177   249
##   Total 3805 1304  5109
## 
## $measure
##    odds ratio with 95% C.I.
##     estimate    lower    upper
##   0  1.00000       NA       NA
##   1  8.12802 6.157525 10.83447
## 
## $p.value
##    two-sided
##     midp.exact fisher.exact   chi.square
##   0         NA           NA           NA
##   1          0 3.858771e-54 4.012304e-64
## 
## $correction
## [1] FALSE
## 
## attr(,&quot;method&quot;)
## [1] &quot;median-unbiased estimate &amp; mid-p exact CI&quot;</code></pre>
<p>Is age a confounding factor in the association between heart disease
and stroke? To answer this question we use logistic regression model
where we adjust for age:</p>
<pre class="r"><code>lr_model &lt;- glm(stroke ~ heart_disease + age, data=dat, family = &quot;binomial&quot;)

summary(lr_model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = stroke ~ heart_disease + age, family = &quot;binomial&quot;, 
##     data = dat)
## 
## Coefficients:
##                 Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)    -7.143068   0.335720 -21.277   &lt;2e-16 ***
## heart_disease1  0.423328   0.185404   2.283   0.0224 *  
## age             0.072424   0.005016  14.438   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1990.3  on 5108  degrees of freedom
## Residual deviance: 1611.3  on 5106  degrees of freedom
## AIC: 1617.3
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<pre class="r"><code>exp(lr_model$coefficients)</code></pre>
<pre><code>##    (Intercept) heart_disease1            age 
##   0.0007903236   1.5270352721   1.0751114927</code></pre>
<p><strong>After adjusting for age, the odds ratio for stroke in
individuals with heart disease decreased from 4.71 to 1.53, indicating
substantial confounding by age. This suggests that age is a confounding
factor in the association between heart disease and stroke.</strong></p>
<p>The same analysis holds for hypertension. <strong>After adjusting for
age, the odds ratio for stroke in individuals with hypertension
decreased from 3.7 to 1.6. This suggests that age is a confounding
factor in the association between hypertension and stroke.</strong></p>
<pre class="r"><code>lr_model &lt;- glm(stroke ~ hypertension + age, data=dat, family = &quot;binomial&quot;)

summary(lr_model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = stroke ~ hypertension + age, family = &quot;binomial&quot;, 
##     data = dat)
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   -7.186365   0.335763 -21.403  &lt; 2e-16 ***
## hypertension1  0.467328   0.160230   2.917  0.00354 ** 
## age            0.072514   0.004994  14.520  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1990.3  on 5108  degrees of freedom
## Residual deviance: 1608.2  on 5106  degrees of freedom
## AIC: 1614.2
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<pre class="r"><code>exp(lr_model$coefficients)</code></pre>
<pre><code>##   (Intercept) hypertension1           age 
##  0.0007568349  1.5957249687  1.0752082175</code></pre>
<p>Now, how about the gender? <strong>There is no association between
gender and the occurrence of stroke.</strong></p>
<pre class="r"><code>tbl_gender &lt;- table(dat$stroke, dat$gender)

oddsratio(tbl_gender)</code></pre>
<pre><code>## $data
##        
##         Female Male Total
##   0       2853 2007  4860
##   1        141  108   249
##   Total   2994 2115  5109
## 
## $measure
##    odds ratio with 95% C.I.
##     estimate    lower    upper
##   0 1.000000       NA       NA
##   1 1.089201 0.840725 1.407353
## 
## $p.value
##    two-sided
##     midp.exact fisher.exact chi.square
##   0         NA           NA         NA
##   1  0.5160089     0.552823  0.5163019
## 
## $correction
## [1] FALSE
## 
## attr(,&quot;method&quot;)
## [1] &quot;median-unbiased estimate &amp; mid-p exact CI&quot;</code></pre>
</div>
<div id="attributable-risk" class="section level4">
<h4><strong>Attributable Risk</strong></h4>
<p>Since we are dealing with a cross-sectional data and risk is a
time-related concept, in what follows we talk about the prevalence of
disease. What percentage of the stroke cases in the hypertension
individuals can be attributed to hypertension? To answer this question
we need to compute attributable risk percent: <strong>70% of strokes in
hypertension individuals could be attributed to
hypertension.</strong></p>
<pre class="r"><code>total_hypert &lt;- sum(dat$hypertension==1)
total_no_hypert &lt;- sum(dat$hypertension==0)

stroke_no_hypertension &lt;- sum((dat$hypertension==0) &amp; (dat$stroke==1))
stroke_with_hypertension &lt;- sum((dat$hypertension==1) &amp; (dat$stroke==1))

par &lt;- round(((stroke_with_hypertension/total_hypert)-(stroke_no_hypertension/total_no_hypert))*100 / (stroke_with_hypertension/total_hypert))

cat(&quot;Attributable risk percent due to hypertension is&quot;, par, &quot;.&quot;)</code></pre>
<pre><code>## Attributable risk percent due to hypertension is 70 .</code></pre>
<p>What percentage of the stroke cases in individuals above the age of
60 can be attributed to their age? <strong>86% of strokes in individuals
above the age of 60 could be attributed to their age.</strong></p>
<pre class="r"><code>total_above_60 &lt;- sum(stroke_age$age_group==1)
total_below_60 &lt;- sum(stroke_age$age_group==0)

stroke_above_60 &lt;- sum((stroke_age$age_group==1) &amp; (dat$stroke==1))
stroke_below_60 &lt;- sum((stroke_age$age_group==0) &amp; (dat$stroke==1))

par &lt;- round(((stroke_above_60/total_above_60)-(stroke_below_60/total_below_60))*100 / (stroke_above_60/total_above_60))

cat(paste0(&quot;Attributable risk percent due to being above 60 is &quot;, par, &quot;%.&quot;))</code></pre>
<pre><code>## Attributable risk percent due to being above 60 is 86%.</code></pre>
<p>What percentage of the stroke cases in the data can be attributed to
hypertension? To answer this question we need to compute population
attributable risk percent. The computation below shows that only
<strong>19% of all stroke cases could be attributed to
hypertension</strong>. In a similar vein, only <strong>14% of all stroke
cases could be attributed to heart disease</strong>.</p>
<pre class="r"><code>n &lt;- nrow(dat)
total_stroke &lt;- sum(dat$stroke==1)
no_hypert &lt;- sum(dat$hypertension==0)
stroke_no_hypertension &lt;- sum((dat$hypertension==0) &amp; (dat$stroke==1))

no_heart_disease &lt;- sum(dat$heart_disease==0)
stroke_no_heart_disease &lt;- sum((dat$heart_disease==0) &amp; (dat$stroke==1))


par &lt;- round(((total_stroke/n)-(stroke_no_hypertension/no_hypert))*100/(total_stroke/n))
par2 &lt;- round(((total_stroke/n)-(stroke_no_heart_disease/no_heart_disease))*100/(total_stroke/n))

cat(&quot;Population attributable risk percent due to hypertension is&quot;, par, &quot;and population attributable risk percent due to heart disease is&quot;, par2, &quot;.&quot;)</code></pre>
<pre><code>## Population attributable risk percent due to hypertension is 19 and population attributable risk percent due to heart disease is 14 .</code></pre>
</div>
<div id="adjusted-odds-ratio" class="section level4">
<h4><strong>Adjusted Odds Ratio</strong></h4>
<p>Logistic regression models the log odds of stroke as a linear
combination of variables. It allows us to see the effect of each
covariate on the odds of stroke while adjusting for all the other
covariates. The result below shows that the age, average glucose level,
and hypertension are significant covariates for stroke, but heart
disease is not (because the age is the confounding factor), after
adjusting for other variables. In other words, <strong>older
individuals, individuals with hypertension, and individuals with higher
glucose levels are at increased risk of stroke.</strong></p>
<pre class="r"><code>lr_model &lt;- glm(stroke ~ age + gender + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level  + smoking_status, data = dat, family = &quot;binomial&quot;)

summary(lr_model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = stroke ~ age + gender + hypertension + heart_disease + 
##     ever_married + work_type + Residence_type + avg_glucose_level + 
##     smoking_status, family = &quot;binomial&quot;, data = dat)
## 
## Coefficients:
##                              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)                 -6.759375   0.752742  -8.980  &lt; 2e-16 ***
## age                          0.074639   0.005729  13.029  &lt; 2e-16 ***
## genderMale                   0.012440   0.141848   0.088 0.930114    
## hypertension1                0.404983   0.164428   2.463 0.013779 *  
## heart_disease1               0.279113   0.191054   1.461 0.144040    
## ever_marriedYes             -0.183344   0.225363  -0.814 0.415902    
## work_typeGovt_job           -0.929849   0.821005  -1.133 0.257393    
## work_typeNever_worked      -10.323844 309.466716  -0.033 0.973387    
## work_typePrivate            -0.787666   0.805106  -0.978 0.327907    
## work_typeSelf-employed      -1.164861   0.826428  -1.410 0.158683    
## Residence_typeUrban          0.083337   0.138338   0.602 0.546897    
## avg_glucose_level            0.004053   0.001174   3.451 0.000558 ***
## smoking_statusnever smoked  -0.206935   0.175938  -1.176 0.239524    
## smoking_statussmokes         0.112117   0.215307   0.521 0.602553    
## smoking_statusUnknown       -0.072984   0.208372  -0.350 0.726145    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1990.3  on 5108  degrees of freedom
## Residual deviance: 1581.2  on 5094  degrees of freedom
## AIC: 1611.2
## 
## Number of Fisher Scoring iterations: 14</code></pre>
<p>To interpret the effect of age, hypertension, and average glucose
level on stroke, we need to exponentiate the coefficients. We see that
<strong>each additional year is associated with the ~7.7% increase in
the odds of stroke. Each additional average glucose level is associated
with the ~0.4% increase in the odds of stroke. People with hypertension
have ~1.5 times the odds of stroke compared to those without
hypertension.</strong></p>
<pre class="r"><code>coeffs &lt;- lr_model$coefficients

coeffs_sign &lt;- coeffs[names(coeffs) %in% c(&quot;age&quot;, &quot;hypertension1&quot;, &quot;avg_glucose_level&quot;)]

exp(coeffs_sign)</code></pre>
<pre><code>##               age     hypertension1 avg_glucose_level 
##          1.077495          1.499276          1.004061</code></pre>
</div>
</div>
</div>
</div>
<div id="task-three-evaluate-and-select-prediction-models"
class="section level1">
<h1>Task Three: Evaluate and select prediction models</h1>
<div id="evaluation-measures" class="section level3">
<h3><strong>Evaluation Measures</strong></h3>
<p>This is a problem with high class imbalance: only around 5 percent of
observations are stroke cases. Thus it is important to choose relevant
evaluation measures. Without any learning, without even considering any
input variables, predicting no stroke would achieve a very high accuracy
result. Thus we concentrate on <strong>sensitivity</strong> and
<strong>specificity</strong> first. <strong>Sensitivity measures the
proportion of individuals correctly identified as stroke cases among
those who actually have stroke, and specificity measures the proportion
of individuals correctly identified as healthy among those who are
actually healthy.</strong> These measures take values between O and 1.
Now, if all individuals are predicted as non-stroke ones, then the
specificity will be 1 since there is no false positives. However,
sensitivity in this case will be 0, since 0 stroke case, i.e., 0 true
positive is identified.</p>
<p>Sensitivity is also known as recall. Precision computes the fraction
of true positives in all positively predicted outputs (i.e., true and
false positives). The <strong>F1-measure</strong> is the harmonic mean
of precision and recall, it balances precision and recall. But notice
that if the specificity is 1 and the sensitivity is 0, then the
precision is undefined, since there is neither true positives or false
positives predicted. Thus, F1 measure is not defined.</p>
<p>Next, we consider the <strong>area under the ROC curve</strong>. This
curve draws sensitivity against false positive rate (1-specificity) at
every cut-off threshold level for predicted probability values, and the
larger the area under this curve the better the predictive performance
of algorithm.</p>
<p>Finally, we consider <strong>Matthews correlation coefficient. The
value of this metric is computed based on the entire confusion matrix,
i.e., it takes into account true positives, false positives, true
negatives and false negatives. It can be considered the most suitable
measure in the class-imbalance setting.</strong></p>
<p>Now, since we are dealing with the class-imbalance problem, we can
choose cut-off threshold values below the standard 0.5 level. This will
output different values for sensitivity, specificity, F1-measure and
Matthews coefficient. Our strategy is based on the Youden’s index, i.e.,
the threshold that maximizes sensitivity + specificity - 1.</p>
</div>
<div id="missing-values-data-splitting" class="section level3">
<h3><strong>Missing Values, Data Splitting</strong></h3>
<p>We can impute the missing values using the k-nearest neighbour
method. It means that the individuals having similar characteristics
will have similar body mass index. But it is questionable from precision
medicine point of view. We can as well remove these individuals from the
data set on the basis of biological differences between individuals even
with very similar characteristics.</p>
<p>We first split the data into the 75%/25% training and test sets using
rsample package of tidymodels framework. <strong>We do stratification
based on the stroke variable to ensure that the proportion of stroke
cases is similar across these splits.</strong> In what follow, we use
set.seed to ensure reproducibility.</p>
<pre class="r"><code>set.seed(123)

split1 &lt;- initial_split(dat, prop = 0.7, strata = stroke)
dat_train &lt;- training(split1)
dat_cv &lt;- vfold_cv(dat_train)

temp &lt;- testing(split1)

split2 &lt;- initial_split(temp, prop = 0.5, strata = stroke)
dat_val &lt;- training(split2)
dat_test &lt;- testing(split2)
ground_truth &lt;- factor(dat_test$stroke, levels = c(1, 0))</code></pre>
</div>
<div id="predictive-modeling" class="section level3">
<h3><strong>Predictive Modeling</strong></h3>
<p>We use logistic regression, also models of higher capacity, capable
of learning non-linear decision boundaries, such as random forest and an
improved version of the gradient boosting method, extreme gradient
boosting. The latter models are particularly suitable for data with many
categorical variables.</p>
<p>We train logistic regression with and without SMOTE, and the
remaining models with SMOTE. The latter generates synthetic samples for
the rare class based on the k-nearest neighbour method. The proportion
of synthetic examples can be controlled thanks to the over_ratio option
in the recipe function. The number of synthetic samples generated affect
sensitivity and specificity: higher number of synthetic samples will
improve sensitivity, and degrade specificity.</p>
<p>We use parsnip package of tidymodels to train logistic regression
model, random forest and xgboost and we perform grid search for
hyperparameter tuning.</p>
<div id="logistic-regression" class="section level4">
<h4><strong>Logistic regression</strong></h4>
<pre class="r"><code>dat_recipe &lt;- recipe(stroke ~ ., data = dat_train) %&gt;% step_impute_knn(all_predictors())

lr_model &lt;- logistic_reg() %&gt;%  set_engine(&quot;glm&quot;) %&gt;% set_mode(&quot;classification&quot;) 

lr_workflow &lt;- workflow() %&gt;% add_model(lr_model) %&gt;% add_recipe(dat_recipe)

lr_fit &lt;- fit(lr_workflow, data = dat_train)

###------threshold selection

val_probs &lt;- predict(lr_fit, dat_val, type = &quot;prob&quot;)$.pred_1
val_truth &lt;- factor(dat_val$stroke, levels = c(1, 0))

threshold_info &lt;- best_threshold(val_truth, val_probs)</code></pre>
<pre><code>## Setting levels: control = 1, case = 0</code></pre>
<pre><code>## Setting direction: controls &gt; cases</code></pre>
<pre class="r"><code>lr_thresh1 &lt;- threshold_info$threshold

###------apply to test set

lr_probs1 &lt;- predict(lr_fit, dat_test, type = &quot;prob&quot;)

lr_result1 &lt;- evaluate_model_fit(lr_thresh1, ground_truth, lr_probs1$.pred_1)</code></pre>
<p>and train with SMOTE:</p>
<pre class="r"><code>dat_smote_recipe &lt;- recipe(stroke ~ ., data = dat_train) %&gt;%
  step_impute_knn(all_predictors()) %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;% 
  step_smote(stroke, over_ratio = 0.7)

lr_model2 &lt;- logistic_reg() %&gt;%  set_engine(&quot;glm&quot;) %&gt;% set_mode(&quot;classification&quot;) 

lr_workflow2 &lt;- workflow() %&gt;% add_model(lr_model2) %&gt;% add_recipe(dat_smote_recipe) ## use smote data recipe

lr_fit2 &lt;- fit(lr_workflow2, data = dat_train)

###------threshold selection

val_probs &lt;- predict(lr_fit2, dat_val, type = &quot;prob&quot;)$.pred_1
val_truth &lt;- factor(dat_val$stroke, levels = c(1, 0))

threshold_info &lt;- best_threshold(val_truth, val_probs)</code></pre>
<pre><code>## Setting levels: control = 1, case = 0</code></pre>
<pre><code>## Setting direction: controls &gt; cases</code></pre>
<pre class="r"><code>lr_thresh2 &lt;- threshold_info$threshold

###------apply to test set

lr_probs2 &lt;- predict(lr_fit2, dat_test, type = &quot;prob&quot;)
lr_result2 &lt;- evaluate_model_fit(lr_thresh2, ground_truth, lr_probs2$.pred_1)</code></pre>
</div>
<div id="random-forest" class="section level4">
<h4><strong>Random Forest</strong></h4>
<pre class="r"><code>rf_model &lt;- rand_forest(
  mtry = tune(),    # Number of predictors sampled at each split
  trees = tune(),   # Number of trees in the forest
  min_n = tune()    # Minimum number of data points in a node to proceed with a split
) %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_mode(&quot;classification&quot;)

rf_workflow &lt;- workflow() %&gt;%
  add_recipe(dat_recipe) %&gt;%
  add_model(rf_model)

rf_grid &lt;- expand.grid(
  mtry = c(2, 3, 5, 7),
  trees = c(100, 300),
  min_n = c(5, 10)
)

rf_tune_results &lt;- rf_workflow %&gt;% 
    tune_grid(resamples = dat_cv, 
              grid = rf_grid, 
              metrics = metric_set(roc_auc)
              )

param_final &lt;- rf_tune_results %&gt;% select_best(metric = &quot;roc_auc&quot;)

rf_workflow_final &lt;- rf_workflow %&gt;% finalize_workflow(param_final)

rf_fit &lt;- rf_workflow_final %&gt;% fit(data = dat_train)

###-------------Threshold selection on validation set

rf_probs_val &lt;- predict(rf_fit, dat_val, type = &quot;prob&quot;)$.pred_1

threshold_info &lt;- best_threshold(val_truth, rf_probs_val)</code></pre>
<pre><code>## Setting levels: control = 1, case = 0</code></pre>
<pre><code>## Setting direction: controls &gt; cases</code></pre>
<pre class="r"><code>rf_thresh &lt;- threshold_info$threshold

###--------------Apply to test data

rf_probs_test &lt;- predict(rf_fit, dat_test, type = &quot;prob&quot;)$.pred_1

rf_result &lt;- evaluate_model_fit(rf_thresh, ground_truth, rf_probs_test)</code></pre>
</div>
<div id="extreme-gradient-boosting" class="section level4">
<h4><strong>Extreme Gradient Boosting</strong></h4>
<pre class="r"><code>xgb_spec &lt;- boost_tree(
  tree_depth = tune(),         # Maximum tree depth
  learn_rate = tune(),         # Learning rate (eta)
) %&gt;% set_mode(&quot;classification&quot;) %&gt;%  set_engine(&quot;xgboost&quot;, eval_metric = &quot;auc&quot;)       

wflow &lt;- workflow() %&gt;% add_recipe(dat_smote_recipe) %&gt;% add_model(xgb_spec)

# Here we use grid_regular() from the dials package to create a regular grid.
grid_vals &lt;- grid_regular(
  tree_depth(range = c(3, 9)),
  learn_rate(range = c(0.01, 0.3)),
  levels = 3
)

tune_res &lt;- tune_grid(
  wflow,
  resamples = dat_cv,
  grid = grid_vals,
  metrics = metric_set(roc_auc)
)

# Collect and inspect the results.
#collect_metrics(tune_res)

# Select the best hyperparameter configuration based on auc.
best_res &lt;- select_best(tune_res, metric=&quot;roc_auc&quot;)

# Finalize the workflow with the best parameters.
final_wflow &lt;- finalize_workflow(wflow, best_res)

xgb_fit &lt;- final_wflow %&gt;% fit(data = dat_train)

###------------------- Threshold selection

xgb_probs_val &lt;- predict(xgb_fit, dat_val, type = &quot;prob&quot;)$.pred_1

threshold_info &lt;- best_threshold(val_truth, xgb_probs_val)</code></pre>
<pre><code>## Setting levels: control = 1, case = 0</code></pre>
<pre><code>## Setting direction: controls &gt; cases</code></pre>
<pre class="r"><code>xgb_thresh &lt;- threshold_info$threshold

###----------------- Apply to test data

xgb_probs_test &lt;- predict(xgb_fit, dat_test, type = &quot;prob&quot;)$.pred_1

xgb_result &lt;- evaluate_model_fit(xgb_thresh, ground_truth, xgb_probs_test)</code></pre>
</div>
</div>
<div id="performance-comparison-of-models" class="section level3">
<h3><strong>Performance Comparison of Models</strong></h3>
<p><strong>According to Matthiews correlation coefficient, all methods
perform weakly (better than random)</strong>. Their performances are
comparable, but logistic regression with smote stands out in terms of
AUC, F1-measure, Matthews coefficient and specificity. We see that
generating synthetic examples overall improved the predictive
performance of logistic regression model at the cost of sensitivity.</p>
<pre class="r"><code>results_df &lt;- tribble(
  ~Model,         ~Sensitivity, ~Specificity, ~F1,       ~AUC,      ~Matthews,
  &quot;Logistic Reg&quot;, lr_result1$sensi, lr_result1$speci, lr_result1$f1, lr_result1$aucroc, lr_result1$matcc,
  &quot;Logistic Reg Smote&quot;, lr_result2$sensi, lr_result2$speci, lr_result2$f1, lr_result2$aucroc, lr_result2$matcc,
  &quot;Random Forest Smote&quot;, rf_result$sensi, rf_result$speci, rf_result$f1, rf_result$aucroc, rf_result$matcc,
  &quot;XGBoost Smote&quot;,       xgb_result$sensi, xgb_result$speci, xgb_result$f1, xgb_result$aucroc, xgb_result$matcc
)


results_long &lt;- results_df %&gt;% pivot_longer(cols = -Model, names_to = &quot;Metric&quot;, values_to = &quot;Value&quot;)


ggplot(results_long, aes(x = Metric, y = Value, fill = Model)) +
  geom_bar(stat = &quot;identity&quot;, position = position_dodge(), width = 0.7) +
  scale_fill_brewer(palette = &quot;Set2&quot;) +
  scale_color_brewer(palette = &quot;Set2&quot;) +
  labs(
    title = &quot;Performance comparison&quot;,
    y = &quot;Value&quot;,
    x = &quot;&quot;
  ) +
  theme_minimal()+     
  theme(plot.title = element_text(hjust = 0.5, size = 16))</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/unnamed-chunk-14-1.png" width="960" /></p>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/unnamed-chunk-15-1.png" width="2400" /></p>
<p>From the confusion matrix, we see that a lot of individuals are
predicted as the stroke case, since we chose lower cut-off threshold
from the ROC curve. Logistic regression has smaller number of false
positives.</p>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/unnamed-chunk-16-1.png" width="1440" /></p>
</div>
<div id="incorrect-predictions-of-logistic-regression"
class="section level3">
<h3><strong>Incorrect predictions of logistic regression</strong></h3>
<p>Now let us look at the distribution of incorrect predictions, in
particular false positives, for logistic regression (trained with
smote). We consider important predictors of stroke such as age and
average glucose level. Since the age is a very important risk factor for
stroke, as expected, there are many false positives for older age. There
are many false positives across all values of average glucose: this is
expected since the class conditional distributions of this variable,
i.e., the distributions of average glucose level conditioned on the
stroke variable, overlap.</p>
<pre class="r"><code>preds &lt;- lr_result2$pred_class

dat_test_preds &lt;- dat_test

dat_test_preds$preds &lt;- factor(preds, levels=c(1,0))

dat_test_preds$outcome &lt;- with(dat_test_preds, ifelse(
  stroke == 1 &amp; preds == 1, &quot;True Positive&quot;,
  ifelse(stroke == 0 &amp; preds == 0, &quot;True Negative&quot;,
  ifelse(stroke == 0 &amp; preds == 1, &quot;False Positive&quot;, &quot;False Negative&quot;))))


p1 &lt;- plot_wrong_predictions(dat_test_preds, &quot;age&quot;, &quot;Age&quot;)

p2 &lt;- plot_wrong_predictions(dat_test_preds, &quot;avg_glucose_level&quot;, &quot;Average glucose level&quot;)


grid.arrange(p1, p2, ncol=2)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/incorrect-preds-1.png" width="1440" /></p>
<p><strong>Positive predictive value and negative predictive value of
logistic regression</strong></p>
<p>The positive predictive value (PPV) is the probability that an
individual predicted to have stroke truly has stroke, while the negative
predictive value (NPV) is the probability that an individual predicted
not to have stroke truly does not have stroke. Considering our logistic
regression model as a diagnostic tool, the probability that an
individual predicted to have stroke has stroke is 17% and the
probability that an individual predicted not to have stroke does not
have stroke is 98%.</p>
<pre class="r"><code>cm &lt;- confusionMatrix(lr_result2$pred_class, ground_truth, positive = &quot;1&quot;)
metrics &lt;- cm$byClass
ppv &lt;- metrics[3]
npv &lt;- metrics[4]

cat(&quot;Positive predicted value is&quot;, round(ppv,2), &quot;and negative predicted value is&quot;, round(npv, 2))</code></pre>
<pre><code>## Positive predicted value is 0.17 and negative predicted value is 0.98</code></pre>
</div>
</div>
<div id="task-four-deploy-the-prediction-model" class="section level1">
<h1>Task Four: Deploy the prediction model</h1>
<p>We train logistic regression model on the entire dataset, where we
impute the missing values of BMI and generate synthetic examples of the
rare class. We save the trained model.</p>
<pre class="r"><code>final_data &lt;- dat

final_recipe &lt;- recipe(stroke ~ ., data = final_data) %&gt;%
  step_impute_knn(all_predictors()) %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;% 
  step_smote(stroke, over_ratio = 0.5)


final_workflow &lt;- workflow() %&gt;% 
  add_model(lr_model2) %&gt;% 
  add_recipe(final_recipe)


final_fit &lt;- fit(final_workflow, data = final_data)

saveRDS(final_fit, file = &quot;stroke_model.rds&quot;)</code></pre>
<p>Use this model to predict stroke probability in shiny interface:</p>
<p><img src="shiny.png" width="60%" /></p>
</div>
<div id="task-five-findings-and-conclusions" class="section level1">
<h1>Task Five: Findings and Conclusions</h1>
<p>The prevalence of stroke cases is ~5%, making this a highly
imbalanced dataset from a classification perspective. Data visualization
revealed considerable overlap between the class-conditional
distributions of important variables such as body mass index, average
glucose level, heart disease, and age. This indicates that no single
covariate is clearly discriminative for stroke classification. From
precision medicine point of view, incorporating family history, ethnic
origin, other environmental factors such as geographic location,
socioeconomic status, as well as genomic profile of individuals would
improve the classification of stroke cases. Importantly, since this is a
cross-sectional data, although obese individuals with hypertension is a
non-stroke case in the dataset, they are at higher risk of stroke as
they age.</p>
<p>Statistical analysis showed that age is a particularly important risk
factor: the majority of stroke cases occur above the age of ~60. Other
important predictors are hypertension and average glucose level. In
contrast, stroke occurs at similar rates across male and female
individuals.</p>
<p>To address the class imbalance problem, we applied SMOTE to
synthetically generate examples of the stroke class and we used a
lower-than-standard probability threshold (less than 0.5) based in
Youden’s index to classify strokes. These manipulations increase model
sensitivity at the cost of increased number of false positives. We used
a parametric method such as logistic regression, and tree-based models
capable of learning highly non-linear decision boundaries and well
adapted to data with many categorical variables: random forest and
gradient boosting. All models are comparable regarding their predictive
performance: they are better than a random guess according to Matthews
correlation coefficient which is computed based on the whole confusion
matrix. However logistic regression stood out with respect to AUC, F1
measures, Matthews correlation coefficient and specificity.</p>
<p>There is a clear trade-off between sensitivity and specificity:
improving sensitivity results in more false positives and some false
negatives. For example, adjusting the classification threshold alters
the balance between these metrics. This is an important epidemiological
question: what are the consequences of generating many false positives
in order to detect a rare but serious condition like stroke?</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>

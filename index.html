<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Khadija Musayeva" />

<meta name="date" content="2025-04-08" />

<title>Build and deploy a stroke prediction model using R</title>

<script src="Build-deploy-stroke-prediction-model-R_files/header-attrs-2.29/header-attrs.js"></script>
<script src="Build-deploy-stroke-prediction-model-R_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="Build-deploy-stroke-prediction-model-R_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="Build-deploy-stroke-prediction-model-R_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="Build-deploy-stroke-prediction-model-R_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="Build-deploy-stroke-prediction-model-R_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="Build-deploy-stroke-prediction-model-R_files/navigation-1.1/tabsets.js"></script>
<link href="Build-deploy-stroke-prediction-model-R_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="Build-deploy-stroke-prediction-model-R_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div id="header">



<h1 class="title toc-ignore">Build and deploy a stroke prediction model
using R</h1>
<h4 class="author">Khadija Musayeva</h4>
<h4 class="date">2025-04-08</h4>

</div>


<style>
body {
  font-size: 18px;
}
</style>
<div id="about-data-analysis-report" class="section level1">
<h1>About Data Analysis Report</h1>
<p>This RMarkdown file contains the report of the data analysis done for
the project on building and deploying a stroke prediction model in R. It
contains data exploration, data visualization,
statistical/epidemiological and predictive modeling analyses of stroke
dataset. The final report was completed on Tue Apr 8 14:44:01 2025.</p>
<p><strong>Data Description:</strong></p>
<p>According to the World Health Organization (WHO) stroke is the 2nd
leading cause of death globally, responsible for approximately 11% of
total deaths.</p>
<p>This data set is used to predict whether a patient is likely to get
stroke based on the input parameters like gender, age, body mass index,
various diseases, and smoking status. Each row in the data provides
relevant information about the patient.</p>
<div id="data-preprocessing-and-analysis" class="section level2">
<h2>Data preprocessing and analysis</h2>
<div id="install-and-load-packages" class="section level3">
<h3>Install and load packages</h3>
<pre class="r"><code>### we use pacman to install and load the required packages
if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;)
pacman::p_load(&quot;caret&quot;, &quot;data.table&quot;, &quot;DescTools&quot;, &quot;egg&quot;, &quot;epitools&quot;, &quot;GGally&quot;, &quot;ggplot2&quot;, &quot;gridExtra&quot;, &quot;mlbench&quot;, &quot;mltools&quot;, &quot;naniar&quot;, &quot;parsnip&quot;, &quot;pROC&quot;, &quot;ranger&quot;, &quot;reshape2&quot;, &quot;recipes&quot;, &quot;rsample&quot;, &quot;shiny&quot;, &quot;smotefamily&quot;, &quot;themis&quot;,&quot;tidymodels&quot;, &quot;tune&quot;, &quot;viridis&quot;, &quot;workflows&quot;, &quot;yardstick&quot;, &quot;xgboost&quot;)

source(&quot;utils.R&quot;)</code></pre>
</div>
<div id="load-the-data" class="section level3">
<h3>Load the data</h3>
<p>Read the data and check its dimensions:</p>
<pre class="r"><code>dat &lt;- as.data.frame(read.csv(&quot;healthcare-dataset-stroke-data.csv&quot;))
cat(&quot;There are&quot;, nrow(dat), &quot;samples and&quot;, ncol(dat), &quot;input variables in the stroke data.&quot;)</code></pre>
<pre><code>## There are 5110 samples and 12 input variables in the stroke data.</code></pre>
<p>What are the types of these variables?</p>
<pre class="r"><code>sapply(dat, class)</code></pre>
<pre><code>##                id            gender               age      hypertension 
##         &quot;integer&quot;       &quot;character&quot;         &quot;numeric&quot;         &quot;integer&quot; 
##     heart_disease      ever_married         work_type    Residence_type 
##         &quot;integer&quot;       &quot;character&quot;       &quot;character&quot;       &quot;character&quot; 
## avg_glucose_level               bmi    smoking_status            stroke 
##         &quot;numeric&quot;       &quot;character&quot;       &quot;character&quot;         &quot;integer&quot;</code></pre>
<p>To this end, we can also use str function, it outputs types of column
as well as an overview of some values:</p>
<pre class="r"><code>str(dat)</code></pre>
<pre><code>## &#39;data.frame&#39;:    5110 obs. of  12 variables:
##  $ id               : int  9046 51676 31112 60182 1665 56669 53882 10434 27419 60491 ...
##  $ gender           : chr  &quot;Male&quot; &quot;Female&quot; &quot;Male&quot; &quot;Female&quot; ...
##  $ age              : num  67 61 80 49 79 81 74 69 59 78 ...
##  $ hypertension     : int  0 0 0 0 1 0 1 0 0 0 ...
##  $ heart_disease    : int  1 0 1 0 0 0 1 0 0 0 ...
##  $ ever_married     : chr  &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; ...
##  $ work_type        : chr  &quot;Private&quot; &quot;Self-employed&quot; &quot;Private&quot; &quot;Private&quot; ...
##  $ Residence_type   : chr  &quot;Urban&quot; &quot;Rural&quot; &quot;Rural&quot; &quot;Urban&quot; ...
##  $ avg_glucose_level: num  229 202 106 171 174 ...
##  $ bmi              : chr  &quot;36.6&quot; &quot;N/A&quot; &quot;32.5&quot; &quot;34.4&quot; ...
##  $ smoking_status   : chr  &quot;formerly smoked&quot; &quot;never smoked&quot; &quot;never smoked&quot; &quot;smokes&quot; ...
##  $ stroke           : int  1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<p>Most of the variables are categorical. We convert them into
factor.</p>
<pre class="r"><code>dat &lt;- dat[, -1]

dat[, -c(2, 8, 9)] &lt;- lapply(dat[, -c(2, 8, 9)], as.factor)
  
dat$bmi &lt;- as.numeric(dat$bmi)</code></pre>
<p>Let us have a glimpse of the data:</p>
<pre class="r"><code>head(dat)</code></pre>
<pre><code>##   gender age hypertension heart_disease ever_married     work_type
## 1   Male  67            0             1          Yes       Private
## 2 Female  61            0             0          Yes Self-employed
## 3   Male  80            0             1          Yes       Private
## 4 Female  49            0             0          Yes       Private
## 5 Female  79            1             0          Yes Self-employed
## 6   Male  81            0             0          Yes       Private
##   Residence_type avg_glucose_level  bmi  smoking_status stroke
## 1          Urban            228.69 36.6 formerly smoked      1
## 2          Rural            202.21   NA    never smoked      1
## 3          Rural            105.92 32.5    never smoked      1
## 4          Urban            171.23 34.4          smokes      1
## 5          Rural            174.12 24.0    never smoked      1
## 6          Urban            186.21 29.0 formerly smoked      1</code></pre>
</div>
<div id="analysis-of-missing-values" class="section level3">
<h3><strong>Analysis of missing values</strong></h3>
<p>To analyze the missing values we use the naniar package.</p>
<p>The following plot shows that 4 percent of bmi values, or 201 of
observations have bmi values missing.</p>
<pre class="r"><code>vis_miss(dat)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/miss-plot-1.png" width="672" /></p>
<p>Are these missing values distributed randomly? To this end, we look
at the distribution of missing values with respect to the variables of
interest. For instance, regarding gender, there are more missing values
for males than for females, and regarding the output variable of
interest, almost ~16 percent of BMI values for stroke individuals are
missing which is at most 4 percent for non-stroke individuals. So these
values are missing at random.</p>
<pre class="r"><code>p1 &lt;- plot_missingness_distribution(dat, &quot;gender&quot;)

p2 &lt;- plot_missingness_distribution(dat, &quot;stroke&quot;)

grid.arrange(p1, p2, ncol = 2)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/miss-distribution-plot-1.png" width="1440" /></p>
</div>
<div id="visualization" class="section level3">
<h3><strong>Visualization</strong></h3>
<p>The following plot illustrates the distribution of categorical
variables.</p>
<p>We see that in this study there are more females than males, more
ever-married individuals, more individuals working in the private sector
than the remaining ones. There are almost the same number of
observations in rural and urban categories.</p>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/cat-plot-1.png" width="864" /></p>
<p><strong>The prevalence of stroke cases across all categorical
variables.</strong></p>
<p>We see that both females and males are equally affected by this
medical condition. There are more stroke cases among individuals with
hypertension, and heart disease. More formerly-smoked individuals have
stroke than the rest, however, there are a lot of individuals with the
unknown smoking status, which makes this analysis biased. Age may be a
confounding variable here. For instance, the majority of formerly-smoked
individuals, or ever-married individuals are relatively older
individuals (above the age of 40).</p>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/stroke-plot-1.png" width="864" /></p>
<p>We use <strong>Cramer’s V</strong> to measure associations between
multiple categories. It ranges from 0 (no association) to 1 (perfect
association). The following heatmap shows that there is some association
between smoking status and marriage status, smoking status and work
type. This is likely due to the age of individuals as a confounding
variable. Most of the associations, however, are weak.</p>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>What about continuous variables: age, average glucose level, and body
mass index?</p>
<p>Half of the individuals have an average glucose level above the
normal 100 mg/dL threshold, indicating a substantial proportion may be
experiencing hyperglycemia. In particular, there is a cluster of
individuals whose glucose levels are concentrated around ~210 mg/dL.
Body mass index distribution is right skewed with more than half of the
individuals having a body mass index (BMI) above the normal upper limit
of 24.9 kg/m^2, i.e., half of the individuals are overweight or obese.
The distribution of age is almost uniform, it includes all ages from
infants to elderly.</p>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/hist-plots-1.png" width="960" /></p>
<p>The distribution of average glucose level among individuals who
experienced a stroke is visibly multimodal, with a secondary peak above
200. The median average glucose level among stroke cases is around 105.
<strong>While individuals with average glucose levels above ~ 200mg/dL
constitute a small portion of observations, they account for
approximately 25% of all stroke cases,</strong> as the following plot
shows.</p>
<pre class="r"><code>p1 &lt;- ggplot(dat, aes(x = stroke, y = avg_glucose_level, color=stroke)) + 
      geom_boxplot(outlier.shape = NA) +
      geom_jitter(width = 0.2, alpha = 0.6, size = 1.5)+
      scale_fill_brewer(palette = &quot;Set3&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      labs(x = &quot;Stroke&quot;, y = &quot;Average glucose level&quot;, color = &quot;Stroke&quot;) +
      theme_minimal(base_size = 14) +
      theme(legend.position = &quot;none&quot;)  


agl_means &lt;- dat %&gt;% group_by(stroke) %&gt;% summarise(agl_mean = median(avg_glucose_level)) 

p2 &lt;- ggplot(dat, aes(x=avg_glucose_level, fill=stroke)) +  geom_density(alpha=0.4) +
      geom_vline(data = agl_means, aes(xintercept=agl_mean, color=stroke), linetype=&quot;dashed&quot;)+
      labs(x = &quot;Average glucose level&quot;, y = &quot;Density&quot;, color = &quot;stroke&quot;) +
      scale_fill_brewer(palette = &quot;Set3&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      theme_minimal(base_size = 14) 
      #+
      #theme(legend.position = &quot;none&quot;)


grid.arrange(p1, p2, ncol=2)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/glucose-stroke-hist-1.png" width="864" /></p>
<p>For BMI variable there is much overlap between the stroke and
non-stroke cases, but the one for stroke individuals is slightly shifted
to the right.</p>
<pre class="r"><code>p1 &lt;- ggplot(dat, aes(x = stroke, y = bmi, color=stroke)) + 
      geom_boxplot(outlier.shape = NA) +
      geom_jitter(width = 0.2, alpha = 0.6, size = 1.5)+
      scale_fill_brewer(palette = &quot;Set3&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      labs(x = &quot;Stroke&quot;, y = &quot;Body mass index&quot;, color = &quot;Stroke&quot;) +
      theme_minimal(base_size = 14) +
      theme(legend.position = &quot;none&quot;)  


bmi_means &lt;- dat %&gt;% filter(!is.na(dat$bmi)) %&gt;%  group_by(stroke) %&gt;% summarise(bmi_mean = mean(bmi)) 

p2 &lt;- ggplot(dat, aes(x=bmi, fill=stroke)) +  geom_density(alpha=0.4)+
      geom_vline(data = bmi_means, aes(xintercept=bmi_mean, color=stroke), linetype=&quot;dashed&quot;)+
      labs(x = &quot;Body mass index&quot;, y = &quot;Density&quot;, color=&quot;stroke&quot;) +
      scale_fill_brewer(palette = &quot;Set3&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      theme_minimal(base_size = 14)

grid.arrange(p1, p2, ncol=2)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/bmi-stroke-hist-1.png" width="960" /></p>
<p>The following plots show that the age is clearly a risk factor for
stroke: <strong>75% of individuals with stroke are aged above
~60</strong>, whereas <strong>75% of individuals with no stroke are aged
below ~60. For stroke individuals, the distribution is left skewed with
half of strokes happening above the age 70.</strong> There are no stroke
cases between the ages 20 and 30 in the data, but there are 2 children
with stroke. The cases involving children can be considered special
cases.</p>
<pre class="r"><code>p1 &lt;- ggplot(dat, aes(x = stroke, y = age, color=stroke)) + 
      geom_boxplot(outlier.shape = NA) +
      geom_jitter(width = 0.2, alpha = 0.6, size = 1.5)+
      scale_fill_brewer(palette = &quot;Set3&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      labs(x = &quot;Stroke&quot;, y = &quot;Age&quot;, color = &quot;Stroke&quot;) +
      theme_minimal(base_size = 14) +
      theme(legend.position = &quot;none&quot;)  


age_means &lt;- dat %&gt;% group_by(stroke) %&gt;% summarise(age_mean = mean(age)) 

p2 &lt;- ggplot(dat, aes(x=age, fill=stroke)) +  geom_density(alpha=0.4)+
      geom_vline(data = age_means, aes(xintercept=age_mean, color=stroke), linetype=&quot;dashed&quot;)+
      labs(x = &quot;Age&quot;, y = &quot;Density&quot;, color = &quot;Stroke&quot;) +
      scale_fill_brewer(palette = &quot;Set3&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      theme_minimal(base_size = 14) +
      theme(legend.position = &quot;none&quot;)

grid.arrange(p1, p2, ncol=2)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/age-stroke-plot-1.png" width="864" /></p>
<p>Next, we analyze pairwise relationships between these variables.
<strong>We see that for stroke individuals, there is a negative
correlation between the age and the body mass index. This association is
positive for non-stroke cases. This is because the majority of strokes
happens above the age of 70, after which the body mass index starts to
decline.</strong> The association between the average glucose level and
body mass index is higher than it is in the non-stroke case.</p>
<pre class="r"><code>ggpairs(dat, columns=c(2, 8, 9), aes(color=stroke, alpha=0.3),
        lower=list(continuous=&quot;smooth&quot;), diag=list(continuous=&quot;densityDiag&quot;))+
  scale_fill_brewer(palette = &quot;Set3&quot;) +
  scale_color_brewer(palette = &quot;Set2&quot;)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/pairwise-plot-1.png" width="960" /></p>
<p><strong>75%</strong> of the hypertension individuals are above the
age of <strong>~52</strong> and <strong>half</strong> of the
hypertension individuals are above the age of <strong>~62</strong>.
<strong>Stroke appears later in life, regardless of hypertension,
however the hypertension individuals who experienced stroke are
generally older individuals</strong>.</p>
<pre class="r"><code>p1 &lt;- ggplot(dat, aes(x = hypertension, y = age, fill = hypertension)) + 
      geom_boxplot(outlier.shape = NA, position = position_dodge(0.8)) +
      geom_jitter(aes(color=hypertension), width=0.2, alpha = 0.5, size = 1.2) +
      scale_fill_brewer(palette = &quot;Set2&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      labs(x = &quot;Hypertension&quot;, y = &quot;Age&quot;, title = &quot;Age distribution by hypertension&quot;) +
      theme_minimal(base_size = 14)+
      theme(plot.title = element_text(hjust = 0.5, size = 16))


p2 &lt;- ggplot(dat, aes(x = hypertension, y = age, fill = stroke)) + 
      geom_boxplot(outlier.shape = NA, position = position_dodge(0.8)) +
      geom_jitter(aes(color = stroke), 
                  position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.8),
                  alpha = 0.5, size = 1.2) +
      scale_fill_brewer(palette = &quot;Set3&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      labs(x = &quot;Hypertension&quot;, y = &quot;Age&quot;, fill = &quot;Stroke&quot;, color = &quot;Stroke&quot;, 
           title = &quot;Age distribution by hypertension and stroke&quot;) +
      theme_minimal(base_size = 14)+
      theme(plot.title = element_text(hjust = 0.5, size = 16))

grid.arrange(p1, p2, ncol=2)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/pairwise_age_hypertension-1.png" width="1440" /></p>
<p>The median age for heart disease is higher than that for
hypertension: above 70. Similarly, <strong>stroke appears later in life,
regardless of heart disease, however there is less variability in the
age distribution of individuals with heart disease and stroke: they are
predominantly older.</strong></p>
<pre class="r"><code>p1 &lt;- ggplot(dat, aes(x = heart_disease, y = age, fill = heart_disease)) + 
      geom_boxplot(outlier.shape = NA, position = position_dodge(0.8)) +
      geom_jitter(aes(color=heart_disease), width=0.2, alpha = 0.5, size = 1.2) +
      scale_fill_brewer(palette = &quot;Set2&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      labs(x = &quot;Heart disease&quot;, y = &quot;Age&quot;, title = &quot;Age distribution by heart disease&quot;) +
      theme_minimal(base_size = 14)+
      theme(plot.title = element_text(hjust = 0.5, size = 16))


p2 &lt;- ggplot(dat, aes(x = heart_disease, y = age, fill = stroke)) + 
      geom_boxplot(outlier.shape = NA, position = position_dodge(0.8)) +
      geom_jitter(aes(color = stroke), 
                  position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.8),
                  alpha = 0.5, size = 1.2) +
      scale_fill_brewer(palette = &quot;Set3&quot;) +
      scale_color_brewer(palette = &quot;Set2&quot;) +
      labs(
        x = &quot;Heart disease&quot;,
        y = &quot;Age&quot;,
        fill = &quot;Stroke&quot;,
        color = &quot;Stroke&quot;,
        title = &quot;Age distribution by heart disease and stroke&quot;
      ) +
      theme_minimal(base_size = 14)+
      theme(
         plot.title = element_text(hjust = 0.5, size = 16)
      )

grid.arrange(p1, p2, ncol=2)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/pairwise_age_heart_disease-1.png" width="1440" /></p>
<p>The following plot shows the age distribution across combinations of
hypertension and heart diseases. For instance 1_1 indicates the
individuals who had both hypertension and heart disease.</p>
<pre class="r"><code>stroke_hypert_heartd &lt;- dat %&gt;%mutate(hyp_hd = interaction(hypertension, heart_disease, sep = &quot;_&quot;))

ggplot(stroke_hypert_heartd, aes(x = hyp_hd, y = age, fill = stroke)) +
    geom_boxplot(position = position_dodge(width = 0.75), outlier.shape = NA) +
    geom_jitter(aes(color = stroke),  
                position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.75),
                alpha = 0.4, size = 1.2) +
    scale_fill_brewer(palette = &quot;Set3&quot;) +
    scale_color_brewer(palette = &quot;Set2&quot;) +
    labs(
      x = &quot;Hypertension_heart disease&quot;,
      y = &quot;Age&quot;,
      title = &quot;Age distribution by hypertension &amp; heart disease, split by stroke&quot;
    ) +
    theme_minimal(base_size = 14) +
    theme(plot.title = element_text(hjust = 0.5, size = 16))</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/pairwise_age_heartd_hypert-1.png" width="1440" /></p>
</div>
<div id="epidemiological-analysis" class="section level3">
<h3><strong>Epidemiological analysis</strong></h3>
<p>In this section, we will do some statistical analysis of association
between variables.</p>
<div id="odds-ratio" class="section level4">
<h4><strong>Odds ratio</strong></h4>
<p>To compute the odds ratio we use <strong>epitools package</strong>.
The outcome of interest is stroke, and the exposures are: hypertension,
heart disease, average glucose level, age and gender.</p>
<p>The following odds ratio computation shows that the <strong>odds of
stroke in hypertensive individuals is ~3.7 times higher than that in
non-hypertensive individuals</strong>.</p>
<pre class="r"><code>tbl_hypertension &lt;- table(dat$stroke, dat$hypertension)

oddsratio(tbl_hypertension)</code></pre>
<pre><code>## $data
##        
##            0   1 Total
##   0     4429 432  4861
##   1      183  66   249
##   Total 4612 498  5110
## 
## $measure
##    odds ratio with 95% C.I.
##     estimate    lower    upper
##   0 1.000000       NA       NA
##   1 3.701246 2.729655 4.964923
## 
## $p.value
##    two-sided
##      midp.exact fisher.exact   chi.square
##   0          NA           NA           NA
##   1 5.77316e-15 4.549182e-15 6.068123e-20
## 
## $correction
## [1] FALSE
## 
## attr(,&quot;method&quot;)
## [1] &quot;median-unbiased estimate &amp; mid-p exact CI&quot;</code></pre>
<p>Also the <strong>odds of stroke in individuals with heart disease is
~4.71 times higher than that in individuals with no heart
disease.</strong></p>
<pre class="r"><code>tbl_heart_disease &lt;- table(dat$stroke, dat$heart_disease)

oddsratio(tbl_heart_disease)</code></pre>
<pre><code>## $data
##        
##            0   1 Total
##   0     4632 229  4861
##   1      202  47   249
##   Total 4834 276  5110
## 
## $measure
##    odds ratio with 95% C.I.
##     estimate    lower    upper
##   0 1.000000       NA       NA
##   1 4.714026 3.309098 6.601552
## 
## $p.value
##    two-sided
##       midp.exact fisher.exact  chi.square
##   0           NA           NA          NA
##   1 8.881784e-15 7.283093e-15 5.20011e-22
## 
## $correction
## [1] FALSE
## 
## attr(,&quot;method&quot;)
## [1] &quot;median-unbiased estimate &amp; mid-p exact CI&quot;</code></pre>
<p>To do odds ratio analysis with average glucose level, we bin it into
two groups at the level of 180 mg/dL. <strong>Individuals with the
average glucose level &gt;180 have ~4 times higher odds of stroke than
those with the average glucose level ≤180.</strong></p>
<pre class="r"><code>stroke_agl &lt;- dat %&gt;% mutate(agl_group = cut(avg_glucose_level, breaks = c(0, 180, max(dat$avg_glucose_level)))) %&gt;% select(c(stroke, agl_group))

tbl_agl &lt;- table(stroke_agl$stroke, stroke_agl$agl_group)

oddsratio(tbl_agl)</code></pre>
<pre><code>## $data
##        
##         (0,180] (180,272] Total
##   0        4357       504  4861
##   1         170        79   249
##   Total    4527       583  5110
## 
## $measure
##    odds ratio with 95% C.I.
##     estimate    lower   upper
##   0 1.000000       NA      NA
##   1 4.019523 3.017215 5.31477
## 
## $p.value
##    two-sided
##     midp.exact fisher.exact   chi.square
##   0         NA           NA           NA
##   1          0  7.34523e-19 4.660299e-25
## 
## $correction
## [1] FALSE
## 
## attr(,&quot;method&quot;)
## [1] &quot;median-unbiased estimate &amp; mid-p exact CI&quot;</code></pre>
<p>We also consider the odds ratio for body mass index, where we group
the individuals into the normal weight, i.e., below 25 kg/m2, and
overweight, i.e., above 25 kg/m2, categories. The result shows that
<strong>overweight individuals have ~2.24 times higher odds of stroke
than those with normal weight.</strong></p>
<pre class="r"><code>dat_bmi &lt;- dat[!is.na(dat$bmi),]

dat_bmi_stroke &lt;- dat_bmi %&gt;% mutate(g = cut(bmi, breaks = c(0, 25, max(dat_bmi$bmi)))) %&gt;% select(c(stroke, g))

tbl_bmi &lt;- table(dat_bmi_stroke$stroke, dat_bmi_stroke$g)

oddsratio(tbl_bmi)</code></pre>
<pre><code>## $data
##        
##         (0,25] (25,97.6] Total
##   0       1569      3131  4700
##   1         38       171   209
##   Total   1607      3302  4909
## 
## $measure
##    odds ratio with 95% C.I.
##     estimate    lower    upper
##   0  1.00000       NA       NA
##   1  2.24731 1.590984 3.257228
## 
## $p.value
##    two-sided
##       midp.exact fisher.exact   chi.square
##   0           NA           NA           NA
##   1 1.680562e-06 2.447681e-06 4.595335e-06
## 
## $correction
## [1] FALSE
## 
## attr(,&quot;method&quot;)
## [1] &quot;median-unbiased estimate &amp; mid-p exact CI&quot;</code></pre>
<p><strong>Odds of stroke in individuals aged above 60 is ~8.13 times
higher than that in indivduals aged less than 60.</strong> Thus we see
that <strong>age is a very important risk factor</strong>. This result
is <strong>statistically very significant, the confidence interval does
not contain 1</strong>, as the two result reported above.</p>
<pre class="r"><code>stroke_age &lt;- dat %&gt;% mutate(age_group = cut(age, breaks = c(0, 60, max(dat$age)), labels=c(&quot;0&quot;, &quot;1&quot;))) 
tbl_age &lt;- table(stroke_age$stroke, stroke_age$age_group)

oddsratio(tbl_age)</code></pre>
<pre><code>## $data
##        
##            0    1 Total
##   0     3734 1127  4861
##   1       72  177   249
##   Total 3806 1304  5110
## 
## $measure
##    odds ratio with 95% C.I.
##     estimate    lower    upper
##   0 1.000000       NA       NA
##   1 8.130197 6.159177 10.83745
## 
## $p.value
##    two-sided
##     midp.exact fisher.exact   chi.square
##   0         NA           NA           NA
##   1          0 3.741383e-54 3.822487e-64
## 
## $correction
## [1] FALSE
## 
## attr(,&quot;method&quot;)
## [1] &quot;median-unbiased estimate &amp; mid-p exact CI&quot;</code></pre>
<p>Is age a confounding factor in the association between heart disease
and stroke? To answer this question we use logistic regression model
where we adjust for age:</p>
<pre class="r"><code>lr_model &lt;- glm(stroke ~ heart_disease + age, data=dat, family = &quot;binomial&quot;)

summary(lr_model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = stroke ~ heart_disease + age, family = &quot;binomial&quot;, 
##     data = dat)
## 
## Coefficients:
##                 Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)    -7.143430   0.335705 -21.279   &lt;2e-16 ***
## heart_disease1  0.423317   0.185405   2.283   0.0224 *  
## age             0.072429   0.005016  14.440   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1990.4  on 5109  degrees of freedom
## Residual deviance: 1611.3  on 5107  degrees of freedom
## AIC: 1617.3
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<pre class="r"><code>exp(lr_model$coefficients)</code></pre>
<pre><code>##    (Intercept) heart_disease1            age 
##   0.0007900377   1.5270177901   1.0751169789</code></pre>
<p><strong>After adjusting for age, the odds ratio for stroke in
individuals with heart disease decreased from 4.71 to 1.53, indicating
substantial confounding by age. This suggests that age is a confounding
factor in the association between heart disease and stroke.</strong></p>
<p>The same analysis holds for hypertension. <strong>After adjusting for
age, the odds ratio for stroke in individuals with hypertension
decreased from 3.7 to 1.6. This suggests that age is a confounding
factor in the association between hypertension and stroke.</strong></p>
<pre class="r"><code>lr_model &lt;- glm(stroke ~ hypertension + age, data=dat, family = &quot;binomial&quot;)

summary(lr_model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = stroke ~ hypertension + age, family = &quot;binomial&quot;, 
##     data = dat)
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   -7.186714   0.335748 -21.405  &lt; 2e-16 ***
## hypertension1  0.467334   0.160231   2.917  0.00354 ** 
## age            0.072519   0.004994  14.522  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1990.4  on 5109  degrees of freedom
## Residual deviance: 1608.2  on 5107  degrees of freedom
## AIC: 1614.2
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<pre class="r"><code>exp(lr_model$coefficients)</code></pre>
<pre><code>##   (Intercept) hypertension1           age 
##  0.0007565708  1.5957347018  1.0752134553</code></pre>
<p>Now, how about the gender? <strong>There is no association between
gender and the occurrence of stroke.</strong></p>
<pre class="r"><code>tbl_gender &lt;- table(dat$stroke, dat$gender)

oddsratio(tbl_gender)</code></pre>
<pre><code>## $data
##        
##         Female Male Other Total
##   0       2853 2007     1  4861
##   1        141  108     0   249
##   Total   2994 2115     1  5110
## 
## $measure
##    odds ratio with 95% C.I.
##     estimate    lower    upper
##   0 1.000000       NA       NA
##   1 1.089201 0.840725 1.407353
## 
## $p.value
##    two-sided
##     midp.exact fisher.exact chi.square
##   0         NA           NA         NA
##   1  0.5160089    0.5746131  0.7895491
## 
## $correction
## [1] FALSE
## 
## attr(,&quot;method&quot;)
## [1] &quot;median-unbiased estimate &amp; mid-p exact CI&quot;</code></pre>
</div>
<div id="attributable-risk" class="section level4">
<h4><strong>Attributable risk</strong></h4>
<p>What percentage of the stroke cases in the hypertension individuals
can be attributed to hypertension? To answer this question we need to
compute attributable risk percent: <strong>70% of strokes in
hypertension individuals could be attributed to
hypertension.</strong></p>
<pre class="r"><code>total_hypert &lt;- sum(dat$hypertension==1)
total_no_hypert &lt;- sum(dat$hypertension==0)

stroke_no_hypertension &lt;- sum((dat$hypertension==0) &amp; (dat$stroke==1))
stroke_with_hypertension &lt;- sum((dat$hypertension==1) &amp; (dat$stroke==1))

par &lt;- round(((stroke_with_hypertension/total_hypert)-(stroke_no_hypertension/total_no_hypert))*100 / (stroke_with_hypertension/total_hypert))

cat(&quot;Attributable risk percent due to hypertension is&quot;, par, &quot;.&quot;)</code></pre>
<pre><code>## Attributable risk percent due to hypertension is 70 .</code></pre>
<p>What percentage of the stroke cases in individuals above the age of
60 can be attributed to their age? <strong>86% of strokes in individuals
above the age of 60 could be attributed to their age.</strong></p>
<pre class="r"><code>total_above_60 &lt;- sum(stroke_age$age_group==1)
total_below_60 &lt;- sum(stroke_age$age_group==0)

stroke_above_60 &lt;- sum((stroke_age$age_group==1) &amp; (dat$stroke==1))
stroke_below_60 &lt;- sum((stroke_age$age_group==0) &amp; (dat$stroke==1))

par &lt;- round(((stroke_above_60/total_above_60)-(stroke_below_60/total_below_60))*100 / (stroke_above_60/total_above_60))

cat(paste0(&quot;Attributable risk percent due to being above 60 is &quot;, par, &quot;%.&quot;))</code></pre>
<pre><code>## Attributable risk percent due to being above 60 is 86%.</code></pre>
</div>
<div id="population-attributable-risk" class="section level4">
<h4><strong>Population attributable risk</strong></h4>
<p>What percentage of the stroke cases in the data can be attributed to
hypertension? To answer this question we need to compute population
attributable risk percent. The computation below shows that only
<strong>19% of all stroke cases could be attributed to
hypertension</strong>. In a similar vein, only <strong>14% of all stroke
cases could be attributed to heart disease</strong>.</p>
<pre class="r"><code>n &lt;- nrow(dat)
total_stroke &lt;- sum(dat$stroke==1)
no_hypert &lt;- sum(dat$hypertension==0)
stroke_no_hypertension &lt;- sum((dat$hypertension==0) &amp; (dat$stroke==1))

no_heart_disease &lt;- sum(dat$heart_disease==0)
stroke_no_heart_disease &lt;- sum((dat$heart_disease==0) &amp; (dat$stroke==1))


par &lt;- round(((total_stroke/n)-(stroke_no_hypertension/no_hypert))*100/(total_stroke/n))
par2 &lt;- round(((total_stroke/n)-(stroke_no_heart_disease/no_heart_disease))*100/(total_stroke/n))

cat(&quot;Population attributable risk percent due to hypertension is&quot;, par, &quot;and population attributable risk percent due to heart disease is&quot;, par2, &quot;.&quot;)</code></pre>
<pre><code>## Population attributable risk percent due to hypertension is 19 and population attributable risk percent due to heart disease is 14 .</code></pre>
</div>
<div id="adjusted-odds-ratio" class="section level4">
<h4><strong>Adjusted Odds Ratio</strong></h4>
<p>Logistic regression models the log odds of stroke as a linear
combination of variables. It allows us to see the effect of each
covariate on the odds of stroke while adjusting for all the other
covariates. The result below shows that the age, average glucose level,
and hypertension are significant covariates for stroke, but heart
disease is not (because the age is the confounding factor), after
adjusting for other variables. In other words, <strong>older
individuals, individuals with hypertension, and individuals with higher
glucose levels are at increased risk of stroke.</strong></p>
<pre class="r"><code>lr_model &lt;- glm(stroke ~ age + gender + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level  + smoking_status, data = dat, family = &quot;binomial&quot;)

summary(lr_model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = stroke ~ age + gender + hypertension + heart_disease + 
##     ever_married + work_type + Residence_type + avg_glucose_level + 
##     smoking_status, family = &quot;binomial&quot;, data = dat)
## 
## Coefficients:
##                              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)                -6.759e+00  7.527e-01  -8.980  &lt; 2e-16 ***
## age                         7.464e-02  5.729e-03  13.029  &lt; 2e-16 ***
## genderMale                  1.244e-02  1.418e-01   0.088 0.930114    
## genderOther                -1.054e+01  1.455e+03  -0.007 0.994221    
## hypertension1               4.050e-01  1.644e-01   2.463 0.013779 *  
## heart_disease1              2.791e-01  1.911e-01   1.461 0.144040    
## ever_marriedYes            -1.833e-01  2.254e-01  -0.814 0.415902    
## work_typeGovt_job          -9.298e-01  8.210e-01  -1.133 0.257393    
## work_typeNever_worked      -1.032e+01  3.095e+02  -0.033 0.973387    
## work_typePrivate           -7.877e-01  8.051e-01  -0.978 0.327907    
## work_typeSelf-employed     -1.165e+00  8.264e-01  -1.410 0.158683    
## Residence_typeUrban         8.334e-02  1.383e-01   0.602 0.546897    
## avg_glucose_level           4.053e-03  1.174e-03   3.451 0.000558 ***
## smoking_statusnever smoked -2.069e-01  1.759e-01  -1.176 0.239524    
## smoking_statussmokes        1.121e-01  2.153e-01   0.521 0.602553    
## smoking_statusUnknown      -7.298e-02  2.084e-01  -0.350 0.726145    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1990.4  on 5109  degrees of freedom
## Residual deviance: 1581.2  on 5094  degrees of freedom
## AIC: 1613.2
## 
## Number of Fisher Scoring iterations: 14</code></pre>
<p>To interpret the effect of age, hypertension, and average glucose
level on stroke, we need to exponentiate the coefficients. We see that
<strong>each additional year is associated with the ~7.7% increase in
the odds of stroke. Each additional average glucose level is associated
with the ~0.4% increase in the odds of stroke. People with hypertension
have ~1.5 times the odds of stroke compared to those without
hypertension.</strong></p>
<pre class="r"><code>coeffs &lt;- lr_model$coefficients

coeffs_sign &lt;- coeffs[names(coeffs) %in% c(&quot;age&quot;, &quot;hypertension1&quot;, &quot;avg_glucose_level&quot;)]

exp(coeffs_sign)</code></pre>
<pre><code>##               age     hypertension1 avg_glucose_level 
##          1.077495          1.499276          1.004061</code></pre>
</div>
</div>
</div>
</div>
<div id="task-three-evaluate-and-select-prediction-models"
class="section level1">
<h1>Task Three: Evaluate and select prediction models</h1>
<div id="first-thing-first-what-evaluation-measures"
class="section level3">
<h3><strong>First thing first: What evaluation measures?</strong></h3>
<p>This is a problem with high class imbalance: only around 5 percent of
observations are stroke cases. Thus it is important to choose relevant
evaluation measures. Without any learning, without even considering any
input variables, predicting no stroke would achieve a very high accuracy
result. Thus we concentrate on <strong>sensitivity</strong> and
<strong>specificity</strong> first. <strong>Sensitivity measures the
proportion of individuals correctly identified as stroke cases among
those who actually have a stroke, and specificity measures the
proportion of individuals correctly identified as healthy among those
who are actually healthy.</strong> These measures take values between O
and 1. Now, if all individuals are predicted as non-stroke ones, then
the specificity will be 1 since there is no false positives. However,
sensitivity in this case will be 0, since 0 stroke case, i.e., 0 true
positive is identified.</p>
<p>Sensitivity is also known as recall. Precision computes the fraction
of true positives over the sum of true and false positives. The
<strong>F1-measure</strong> is the harmonic mean of precision and
recall, it balances precision and recall. But notice that if the
specificity is 1 and the sensitivity is 0, then the precision is
undefined, since there is neither true positives or false positives
predicted. Thus, F1 measure is not defined.</p>
<p>Next, we consider the <strong>area under the ROC curve</strong>. This
curve draws sensitivity against false positive rate (1-specificity) at
every cut-off threshold level for predicted probability values, and the
larger the area under this curve the better the predictive performance
of algorithm.</p>
<p>Finally, we consider <strong>Matthews correlation coefficient. The
value of this metric is computed based on the entire confusion matrix,
i.e., it takes into account true positives, false positives, true
negatives and false negatives. It can be considered the most suitable
measure in the class-imbalance setting.</strong></p>
<p>Now, since we are dealing with the class-imbalance problem, we can
choose cut-off threshold values below the standard 0.5 level. This will
output different values for sensitivity, specificity, F1-measure and
Matthews coefficient. Our strategy is based on the Youden’s index, i.e.,
the threshold that maximizes sensitivity + specificity - 1.</p>
</div>
<div id="missing-values-data-splitting" class="section level3">
<h3><strong>Missing values, data splitting</strong></h3>
<p>We can impute the missing values using the k-nearest neighbour
method. It means that the individuals having similar characteristics
will have similar body mass index. But it is questionable from precision
medicine point of view. We can as well remove these individuals from the
data set on the basis of biological differences between individuals even
with very similar characteristics.</p>
<p>We first split the data into the 75%/25% training and test sets using
rsample package of tidymodels framework. <strong>We do stratification
based on the stroke variable to ensure that the proportion of stroke
cases is similar across these splits.</strong> In what follow, we use
set.seed to ensure reproducibility.</p>
<pre class="r"><code>set.seed(123)

dat_split &lt;- initial_split(dat, prop = 3/4, strata = stroke)

dat_train &lt;- training(dat_split)

dat_test &lt;- testing(dat_split)
                      
dat_cv &lt;- vfold_cv(dat_train)</code></pre>
</div>
<div id="predictive-modeling" class="section level3">
<h3><strong>Predictive modeling</strong></h3>
<p>We use logistic regression, also models of higher capacity, capable
of learning non-linear decision boundaries, such as random forest and an
improved version of the gradient boosting method, extreme gradient
boosting. The latter models are particularly suitable for data with many
categorical variables.</p>
<p>We train logistic regression with and without SMOTE, and the
remaining models with SMOTE. The latter generates synthetic samples for
the rare class based on the k-nearest neighbour method. The proportion
of synthetic examples can be controlled thanks to the over_ratio option
in the recipe function. The number of synthetic samples generated affect
sensitivity and specificity: higher number of synthetic samples will
improve sensitivity, and degrade specificity.</p>
<p>We use parsnip package of tidymodels to train logistic regression
model and random forest. There is no implementation of extreme gradient
boosting, xgb, in tidymodels, so we use xgboost package. We do a grid
search for hyperparameter tuning.</p>
<div id="logistic-regression" class="section level4">
<h4><strong>Logistic regression</strong></h4>
<pre class="r"><code>ground_truth &lt;- factor(dat_test$stroke, levels = c(1, 0)) # set the ground truth

dat_recipe &lt;- recipe(stroke ~ ., data = dat_train) %&gt;% step_impute_knn(all_predictors())

lr_model &lt;- logistic_reg() %&gt;%  set_engine(&quot;glm&quot;) %&gt;% set_mode(&quot;classification&quot;) 

lr_workflow &lt;- workflow() %&gt;% add_model(lr_model) %&gt;% add_recipe(dat_recipe)

lr_fit &lt;- fit(lr_workflow, data = dat_train)

lr_fit_extracted &lt;- extract_fit_engine(lr_fit)

lr_probs1 &lt;- predict(lr_fit, dat_test, type = &quot;prob&quot;)

lr_result1 &lt;- evaluate_model_fit(ground_truth, lr_probs1$.pred_1)</code></pre>
<pre><code>## Setting levels: control = 1, case = 0</code></pre>
<pre><code>## Setting direction: controls &gt; cases</code></pre>
<p>and train with SMOTE:</p>
<pre class="r"><code>dat_smote_recipe &lt;- recipe(stroke ~ ., data = dat_train) %&gt;%
  step_impute_knn(all_predictors()) %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;% 
  step_smote(stroke, over_ratio = 0.5)

lr_model2 &lt;- logistic_reg() %&gt;%  set_engine(&quot;glm&quot;) %&gt;% set_mode(&quot;classification&quot;) 

lr_workflow2 &lt;- workflow() %&gt;% add_model(lr_model2) %&gt;% add_recipe(dat_smote_recipe) ## use smote data recipe

lr_fit2 &lt;- fit(lr_workflow2, data = dat_train)

lr_fit_extracted2 &lt;- extract_fit_engine(lr_fit2)

lr_probs2 &lt;- predict(lr_fit2, dat_test, type = &quot;prob&quot;)

lr_result2 &lt;- evaluate_model_fit(ground_truth, lr_probs2$.pred_1)</code></pre>
<pre><code>## Setting levels: control = 1, case = 0</code></pre>
<pre><code>## Setting direction: controls &gt; cases</code></pre>
</div>
<div id="random-forest" class="section level4">
<h4><strong>Random forest</strong></h4>
<pre class="r"><code>rf_model &lt;- rand_forest() %&gt;% set_args(mtry = tune()) %&gt;%
  set_engine(&quot;ranger&quot;, importance = &quot;impurity&quot;) %&gt;%
  set_mode(&quot;classification&quot;) 


rf_workflow &lt;- workflow() %&gt;%
  add_recipe(dat_smote_recipe) %&gt;%
  add_model(rf_model)

rf_grid &lt;- expand.grid(mtry = c(2, 3, 4, 5))

rf_tune_results &lt;- rf_workflow %&gt;% tune_grid(resamples = dat_cv, grid = rf_grid, metrics = metric_set(roc_auc))

rf_tune_results %&gt;%  collect_metrics()</code></pre>
<pre><code>## # A tibble: 4 × 7
##    mtry .metric .estimator  mean     n std_err .config             
##   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1     2 roc_auc binary     0.819    10 0.00867 Preprocessor1_Model1
## 2     3 roc_auc binary     0.817    10 0.0109  Preprocessor1_Model2
## 3     4 roc_auc binary     0.809    10 0.0116  Preprocessor1_Model3
## 4     5 roc_auc binary     0.808    10 0.0129  Preprocessor1_Model4</code></pre>
<pre class="r"><code>param_final &lt;- rf_tune_results %&gt;% select_best(metric = &quot;roc_auc&quot;)

rf_workflow &lt;- rf_workflow %&gt;% finalize_workflow(param_final)

rf_fit &lt;- rf_workflow %&gt;% last_fit(dat_split)

#test_performance &lt;- rf_fit %&gt;% collect_metrics()

rf_probs &lt;- as.data.frame(rf_fit$.predictions)$.pred_1

rf_result &lt;- evaluate_model_fit(ground_truth, rf_probs)</code></pre>
<pre><code>## Setting levels: control = 1, case = 0</code></pre>
<pre><code>## Setting direction: controls &gt; cases</code></pre>
</div>
<div id="extreme-gradient-boosting" class="section level4">
<h4><strong>Extreme gradient boosting</strong></h4>
<pre class="r"><code>dat_smote_recipe &lt;- dat_smote_recipe %&gt;% prep()

X_train &lt;- dat_train %&gt;% select(-stroke)
X_test &lt;- dat_test %&gt;% select(-stroke)

X_train &lt;- bake(dat_smote_recipe, X_train)
X_test &lt;- bake(dat_smote_recipe, X_test)

y_train &lt;- as.numeric(dat_train$stroke)-1
y_test  &lt;- as.numeric(dat_test$stroke)-1


X_train_matrix &lt;- as.matrix(X_train)
X_test_matrix &lt;- as.matrix(X_test)

dtrain &lt;- xgb.DMatrix(data = X_train_matrix, label = y_train)
dtest  &lt;- xgb.DMatrix(data = X_test_matrix, label = y_test)


params_list &lt;- expand.grid(
  eta = c(0.01, 0.1, 0.3),
  max_depth = c(3, 6, 9, 12),
  subsample = c(0.3, 0.5, 0.7, 0.9),
  colsample_bytree = c(0.2, 0.5, 0.8)
)

cv_results &lt;- xgb_tune(params_list, dtrain, 100)

cv_summary &lt;- rbindlist(cv_results)

best_params &lt;- cv_summary[which.max(auc)]

### we train xgb with the best hyperparameter configuration on the entire training set.
xgb_final_model &lt;- xgb.train(
  params = as.list(best_params),
  data = dtrain,
  nrounds = best_params$best_iteration,  
  watchlist = list(train = dtrain),
  verbose = 0
  )</code></pre>
<pre><code>## [14:46:50] WARNING: src/learner.cc:767: 
## Parameters: { &quot;auc&quot;, &quot;best_iteration&quot; } are not used.</code></pre>
<pre class="r"><code>xgb_probs &lt;- predict(xgb_final_model, dtest)

xgb_result &lt;- evaluate_model_fit(ground_truth, xgb_probs)</code></pre>
</div>
</div>
<div id="performance-comparison-of-models" class="section level3">
<h3><strong>Performance comparison of models</strong></h3>
<p><strong>According to Matthiews correlation coefficient, all methods
perform weakly (better than random)</strong>. Their performances are
comparable, but logistic regression with smote stands out in terms of
AUC, F1-measure, Matthews coefficient and specificity. We see that
generating synthetic examples overall improved the predictive
performance of logistic regression model at the cost of sensitivity.</p>
<pre class="r"><code>results_df &lt;- tribble(
  ~Model,         ~Sensitivity, ~Specificity, ~F1,       ~AUC,      ~Matthews,
  &quot;Logistic Reg&quot;, lr_result1$sensi, lr_result1$speci, lr_result1$f1, lr_result1$aucroc, lr_result1$matcc,
  &quot;Logistic Reg Smote&quot;, lr_result2$sensi, lr_result2$speci, lr_result2$f1, lr_result2$aucroc, lr_result2$matcc,
  &quot;Random Forest Smote&quot;, rf_result$sensi, rf_result$speci, rf_result$f1, rf_result$aucroc, rf_result$matcc,
  &quot;XGBoost Smote&quot;,       xgb_result$sensi, xgb_result$speci, xgb_result$f1, xgb_result$aucroc, xgb_result$matcc
)


results_long &lt;- results_df %&gt;% pivot_longer(cols = -Model, names_to = &quot;Metric&quot;, values_to = &quot;Value&quot;)


ggplot(results_long, aes(x = Metric, y = Value, fill = Model)) +
  geom_bar(stat = &quot;identity&quot;, position = position_dodge(), width = 0.7) +
  scale_fill_brewer(palette = &quot;Set2&quot;) +
  scale_color_brewer(palette = &quot;Set2&quot;) +
  labs(
    title = &quot;Performance comparison&quot;,
    y = &quot;Value&quot;,
    x = &quot;&quot;
  ) +
  theme_minimal()+     
  theme(plot.title = element_text(hjust = 0.5, size = 16))</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/unnamed-chunk-14-1.png" width="960" /></p>
<pre class="r"><code>p1 &lt;- plot_roc(data.frame(ground_truth, probs=lr_probs2$.pred_1), &quot;Logistic Regression&quot;)

p2 &lt;-  plot_roc(data.frame(ground_truth, probs=rf_probs), &quot;Random Forest&quot;)

p3 &lt;-  plot_roc(data.frame(ground_truth, probs=xgb_probs), &quot;XGB&quot;)

grid.arrange(p1, p2, p3, ncol = 4)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/unnamed-chunk-15-1.png" width="2400" /></p>
<p>From the confusion matrix, we see that a lot of individuals are
predicted as the stroke case, since we chose lower cut-off threshold
from the ROC curve. Logistic regression has smaller number of false
positives.</p>
<pre class="r"><code>p1 &lt;- plot_confusion_matrix(ground_truth, lr_result2$pred_class, round(lr_result2$matcc, 2), &quot;Logistic regression&quot;)

p2 &lt;- plot_confusion_matrix(ground_truth, rf_result$pred_class, round(rf_result$matcc, 2), &quot;Random forest&quot;)

p3 &lt;- plot_confusion_matrix(ground_truth, xgb_result$pred_class, round(xgb_result$matcc, 2), &quot;XGB&quot;)


grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/unnamed-chunk-16-1.png" width="1440" /></p>
</div>
<div id="incorrect-predictions-of-logistic-regression"
class="section level3">
<h3><strong>Incorrect predictions of logistic regression</strong></h3>
<p>Now let us look at the distribution of incorrect predictions, in
particular false positives, for logistic regression (trained with
smote). We consider important predictors of stroke such as age and
average glucose level. Since the age is a very important risk factor for
stroke, as expected, there are many false positives for older age. There
are many false positives across all values of average glucose: this is
expected since the class conditional distributions of this variable,
i.e., the distributions of average glucose level conditioned on the
stroke variable, overlap.</p>
<pre class="r"><code>preds &lt;- lr_result2$pred_class

dat_test_preds &lt;- dat_test

dat_test_preds$preds &lt;- factor(preds, levels=c(1,0))

dat_test_preds$outcome &lt;- with(dat_test_preds, ifelse(
  stroke == 1 &amp; preds == 1, &quot;True Positive&quot;,
  ifelse(stroke == 0 &amp; preds == 0, &quot;True Negative&quot;,
  ifelse(stroke == 0 &amp; preds == 1, &quot;False Positive&quot;, &quot;False Negative&quot;))))


p1 &lt;- plot_wrong_predictions(dat_test_preds, &quot;age&quot;, &quot;Age&quot;)

p2 &lt;- plot_wrong_predictions(dat_test_preds, &quot;avg_glucose_level&quot;, &quot;Average glucose level&quot;)


grid.arrange(p1, p2, ncol=2)</code></pre>
<p><img src="Build-deploy-stroke-prediction-model-R_files/figure-html/incorrect-preds-1.png" width="1440" /></p>
<p><strong>Positive predictive value and negative predictive value of
logistic regression</strong></p>
<p>The positive predictive value (PPV) is the probability that an
individual predicted to have a stroke truly has a stroke, while the
negative predictive value (NPV) is the probability that an individual
predicted not to have a stroke truly does not have a stroke. Considering
our logistic regression model as a diagnostic tool, the probability that
an individual predicted to have a stroke has a stroke is 16% and the
probability that an individual predicted not to have a stroke does not
have a stroke is 98%.</p>
<pre class="r"><code>cm &lt;- confusionMatrix(lr_result2$pred_class, ground_truth, positive = &quot;1&quot;)
metrics &lt;- cm$byClass
ppv &lt;- metrics[3]
npv &lt;- metrics[4]

cat(&quot;Positive predicted value is&quot;, round(ppv,2), &quot;and negative predicted value is&quot;, round(npv, 2))</code></pre>
<pre><code>## Positive predicted value is 0.15 and negative predicted value is 0.98</code></pre>
</div>
</div>
<div id="task-four-deploy-the-prediction-model" class="section level1">
<h1>Task Four: Deploy the prediction model</h1>
<p>We train logistic regression model on the entire dataset, where we
impute the missing values of BMI and generate synthetic examples of the
rare class. We save the trained model.</p>
<pre class="r"><code>final_data &lt;- dat

final_recipe &lt;- recipe(stroke ~ ., data = final_data) %&gt;%
  step_impute_knn(all_predictors()) %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;% 
  step_smote(stroke, over_ratio = 0.5)


final_workflow &lt;- workflow() %&gt;% 
  add_model(lr_model2) %&gt;% 
  add_recipe(final_recipe)


final_fit &lt;- fit(final_workflow, data = final_data)

saveRDS(final_fit, file = &quot;stroke_model.rds&quot;)</code></pre>
<p>Use this model to predict stroke probability in shiny interface:</p>
<p><img src="shiny.png" width="60%" /></p>
</div>
<div id="task-five-findings-and-conclusions" class="section level1">
<h1>Task Five: Findings and Conclusions</h1>
<p>The prevalence of stroke cases is ~5%, making this a highly
imbalanced dataset from a classification perspective. Data visualization
revealed considerable overlap between the class-conditional
distributions of important variables such as body mass index, average
glucose level, heart disease, and age. This indicates that no single
covariate is clearly discriminative for stroke classification. From
precision medicine point of view, incorporating family history, ethnic
origin, other environmental factors such as geographic location,
socioeconomic status, as well as genomic profile of individuals would
improve the classification of stroke cases. Importantly, since this is a
cross-sectional data, although obese individuals with heart condition is
a non-stroke case in the dataset, they are at higher risk of stroke as
they age.</p>
<p>Statistical analysis showed that age is a particularly important risk
factor: the majority of stroke cases occur above the age of ~60. Other
important predictors are hypertension and average glucose level. In
contrast, stroke occurs at similar rates across male and female
individuals.</p>
<p>To address the class imbalance problem, we applied SMOTE to
synthetically generate examples of the stroke class and we used a
lower-than-standard probability threshold (less than 0.5) based in
Youden’s index to classify strokes. These manipulations increase model
sensitivity at the cost of increased number of false positives. We used
a parametric method such as logistic regression, and tree-based models
capable of learning highly non-linear decision boundaries and well
adapted to data with many categorical variables: random forest and
gradient boosting. All models are comparable regarding their predictive
performance: they are better than a random guess according to Matthews
correlation coefficient which is computed based on the whole confusion
matrix. However logistic regression stood out with respect to AUC, F1
measures, Matthews correlation coefficient and specificity.</p>
<p>There is a clear trade-off between sensitivity and specificity:
improving sensitivity results in more false positives and some false
negatives. For example, adjusting the classification threshold alters
the balance between these metrics. This is an important epidemiological
question: what are the consequences of generating many false positives
in order to detect a rare but serious condition like stroke?</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
